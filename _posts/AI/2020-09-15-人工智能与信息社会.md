---
layout: post
title: "人工智能与信息社会mooc"
author: "yuruj"
subtitle: "神经网络"
catelog: true
header-style: text
tags:
  - AI
  - 学习笔记
---

基于决策树和搜索的智能系统



专家系统

博弈树构建

在完全信息零和博弈的条件下，能够构建简单的博弈树

如果在不完全信息、非零和博弈的情况下，博弈树较为复杂

估值函数

对每一个局面给出一个估值

棋盘特征：不同位置，不同时间，价值不同

井字棋的估值函数：自己未来有可能连成直线的行列数减去对方未来有可能连成直线的行列数

选取策略：

根据估值函数选择最优策略

最佳行动就是能够使得下一个状态的评估值最大的行动

进一步改进：采用多步搜索策略 提高搜索的深度 尽量接近搜索过程的终止状态 搜索配合剪枝提高效率 改进估值函数 针对不同棋类添加不同的估值 通过神经网络等方式让AI自己学的策略



最大最小值法：

回合制的游戏

双方都很聪明，采用最优策略

用最大最小值法统一表示

minimax值：表示决策树上节点的估值

一个minimax决策树，包括max节点、min节点、终止节点

对于终止节点，minimax值等于直接对局面的估值

对于MAX节点，选择minimax值最大的子节点的值作为MAX节点的值

对于MIN节点，选择MINIMAX值最小的值作为MIN节点的值

算法过程：

构建决策树

将评估函数应用于子节点

自底向上计算每个节点的minimax值

从根节点选择minimax最大的分支，作为行动策略



AlphaBeta剪枝

算法过程：

开始构建决策树

将估值函数应用于叶子节点

深度优先搜索，传递并更新a，b，节点值

自下到上 自左到右

max节点更新a值（下限），min节点更新b值（下限）

从根节点选择评估值最大的分支，作为行动策略



启发式算法：
 在搜索过程中，启发式算法被定义成一系列额外的规则

经验法则

利用一些特定的知识

它常能发现很不错的解，但也没有办法证明它不会得到较坏的解

它通常可在合理时间解出答案，但也没办法知道它是否每次都可以这样的速度求解

启发性规则反映在额外的估值函数中，和终止局面的估值函数一起计算

重点在于如何设计并实现启发函数，使我们能够更快地获得较优解

深蓝的算法：

AlphaBeta剪枝

残局库（增加搜索深度）

人类对局开局库



蒙特卡洛方法：通过随机采样计算得到近似结果，一种通用的计算方法

蒙特卡洛树搜索（MCTS）

一种通过决策空间中随机采样并根据结果构建决策树来寻找最优策略的方法

决策树的构建：选择、扩张、模拟、反馈

选择、扩张

树策略：从决策树中选择并创建新的叶节点

模拟、反馈

默认策略：从一个非中止状态不断进行游戏并得到一个价值评估

AlphaGo 在MCTS的基础上，通过神经网络进行训练的到更好的树策略和估值函数



基于仿生算法的智能系统



仿生算法

是人类模仿生物的功能与行为进而总结出来用于解决问题的方法

通过模拟自然生物进化或者群体社会行为来进行随机搜索

适用于传统算法难以解决的大规模复杂优化问题

模仿生物进化的遗传算法

模仿蚂蚁寻路的蚁群算法

模仿神经网络系统的人工神经网络

一般算法：基于经验或者已知的规则，针对特定的输入能够计算出确定的输出结构

仿生算法：模仿生物的功能与行为 通过大量演变来逼近结果，演变方法是自然法则：优胜劣汰，适者生存

仿生算法是一类模拟自然生物进化或者群体社会行为的随机搜索方法的统称



基因遗传算法

一种受达尔文自然进化理论启发的搜索启发式算法

反映了自然选择的过程

编码（初始化） 适应度函数 选择 交叉 变异

主要流程：

随机生成初始群体

开始主循环（结束的标准可以是迭代次数，或者适应度达到某个要求）

1、执行策略，计算当前群体中所有个体的适应度

2、从当前群体中，选择精英作为下一代的父母

3、将选出的精英父母配对

4、以极小概率将子代变异

5、将子代个体添加到新群体中

基因遗传算法的应用：函数优化 组合优化

优化是通过改变参数来最大化或最小化目标函数

所有可能解（参数值）的集合构成搜索空间。在这个搜索空间中，有一组点是最佳解决方案，优化的目的就是在搜索空间中找到这样一组点

优点：全局搜索性强

缺点：计算量较大 具有随机性，无法确保解决方案一定最优

图片的适应度：每一个像素点，三个颜色通道分辨计算当前图像A与目标图像B的差值



基于神经网络的智能系统



人工神经网络

一个完整的神经网络由一层输入层、多层隐藏层、一层输出层构成

前馈型神经网络：单向多层结构，整个网络中无反馈

常用于图像识别、检测、分割

反馈型神经网络：是一种从输出到输入具有反馈连接的神经网络，当前的结果收到先前所有的结果的影响

常用于语音、文本处理、问答系统等

神经网络训练算法

以误差为主导的反向传播算法

其本质是通过前向传递传递输入信号直至输出产生误差，再将误差信息反向传播去更新网络权重矩阵

通过这种反馈机制，反馈越多，神经网络学习准确率越高

MNIST数据集准备-著名的手写体数字识别数据集



网络构建

隐藏层用于提取特征

隐藏层主要包括卷积层、全连接层、池化层、归一化指数层、激活层等

卷积层的作用是提供图像的二维特征，通过不同的算子可以检测图像不同边缘

全连接层的作用是将所有特征融合到一起

池化层作用是减少训练参数，是对原始特征信号进行采样 当输入数据过大时，卷积层的计算量就会很大，这是需要减少参数，因此池化层常出现在卷积层之后

激活层：神经元有两种状态。事实上处于不同程度兴奋的神经元传播化学物质也不尽相同

非线形激活层就是决定哪些神经元的活跃程度高，那些年神经元的活跃程度低

归一化指数层的作用就是完成最后输出分类是每个类别概率的计算，通过归一函数，使得最后输出的是十维向量的每一个值加起来总和为1

监督学习：通过一系列带标签的数据训练

损失函数（代价函数）

0-1损失函数

平方损失函数



如何优化参数：

神经网络中的参数是海量的

优化器代表了调整网络参数，使损失函数达到最小的过程

梯度下降算法



反向传播

根据梯度下降的方法，将当前的损失函数反馈给之前各层的神经网络，并调整各层网络参数的权值，这个过程称为反向传播

一个神经元的输入输出可以类比为一个多元一次的线性方程

根据梯度大小不同，调整不同的参数对损失函数影响不同

调整系数w

调整输出y的一个方法是调整系数w

前一层的神经元激活值大，表示越有可能看到某些与2有关的特征

调整激活值大的神经元的参数，影响比调整激活值小的参数要大

调整输入x

调整输出y的另一个方法是调整输入x

增加原始参数w是正值的神经元的激活值

减少原始参数w是负值的神经元的激活值

增加和减少的程度与参数w大小相关

但我们无法直接改变每个神经元的激活值，将每个输出需要改变的大小叠加之后，能够得到总的该变量



监督学习可以用来预测新的样本

分类

支持向量机：寻找最大化样本间隔的边界

分类决策树

神经网络方法

回归

直线拟合（最小二乘法）

例如人脸好看程度评分

线性回归

最邻近方法：使用最相似的训练样本来预测新样本值

神经网络方法

非监督学习

聚类：把这些没有标签的数据分成一个一个组合，就是聚类

Google新闻

鸡尾酒会问题



强化学习：符合学习玩游戏情形

控制一个在某个环境中的主体，通过与环境的互动来改善主题的行为 

试错式学习

效果律：紧接着有利后果的行为更有可能再次发生 不良后果的行为不太可能再次发生

强化学习使得计算机能够像人一样通过不断试错式学习，完全自主掌握一项技能 不需要借鉴人类的经验 具有发展强人工智能的潜力

强化学习的要素：主体 环境 状态 动作 回报

强化学习的主要流程：主题与环境不断的进行交互，产生多次尝试的经验，再利用这些经验去修改自身策略。经过大量迭代学习，最终获得最佳策略

价值判断Q函数

策略：从状态集（所有可能出现的状态）到一个动作集（所有可能采取的动作）的一个对应关系

目的：求得最佳策略

与手写数字识别不同，在强化学习的过程中我们不关系把当前的状态的分为什么类型，而是关心他能否执行最佳行动

状态值函数V只和状态相关，用于某个状态进行估值

状态动作函数Q和状态以及在该状态下采取的动作相关，用于对某个局面状态下采取某个动作进行估值

Q-Learning

强化学习中一种常用算法 基于状态动作函数Q，如果知道了某一状态下每个动作的估值，那么就可以选择估值最好的一个动作去执行了

简单的Q函数表Q-Table

基于神经网络计算Q函数

对于复杂的状态，无法用表格表示，可使用神经网络对Q函数进行建模，其输入为状态，输出为各个动作的评估值。还是选取最高的动作执行。

Q-Learning算法通过学习获得一个状态动作函数

不直接决定主体该采取什么决策，而是提供一个估值参考

如果Q函数较优，可以直接取最大价值来决定动作



初始化一个随机Q函数，从零开始不断学习

如何尝试：在Q函数不够准确的时候，每次尝试该如何选择动作 设计到探索和开发两者的平衡

探索：为了更好学习最佳Q函数而尝试各种情况

开发：直接选择当前认为最佳的动作 再进一步修改新状态下的Q值

探索有利于获得更好的策略

开发有利于测试算法是否有效

greedy策略

一种简单的平衡探索与开发的策略

有概率选取一个随机动作，剩下的情况依然选取Q值最大的动作

概率一般比较小

可以更改概率值从而得到不同的探索和开发的概率

学习流程

初始化Q函数

不断重复每一局游戏

最终得到一个好的Q函数

动作-状态序列

每一局游戏都是一个动作状态序列

下一个状态只和当前的状态+动作有关（马尔可夫性质）

长期回报

除了试错式搜索之外，强化学习的另一个重要的特点是回报的滞后性

当前的状态下的动作所产生的回报不仅取决于下一个状态，还取决于整个序列之后的每一个状态

某些动作产生的当前回报值比较高，但从长远来看，可能并没有那么高

回报率

当前的动作对下一状态的影响时最直接的，对后续状态影响没那么直接

因此我们用一个回报率来平衡下一状态回报和更远状态回报

回报函数

每一次游戏会产生不同的状态动作序列，即每一次对后续状态的回报计算都不相同

我们用后续状态的期望，即所有之后的序列的回报平均值作为回报函数

回报函数值就是Q值

学习过程，每完成一局后，就持续更新Q函数

完成的局数越多，更新的次数就越多，结果也越准确

学习率

既要利用好已经学好的值，也要善于学习新的值

这两者就通过学习率来平衡，一开始学习率可以大一些，最后稳定时学习率可以小一些



ImageNet数据集

利用卷积神经网络进行图像识别

卷积神经网络是一种为了处理二维输入数据而特殊设计的多层人工神经网络

不仅关注了全局特征，更利用了图像识别领域非常重要的局部特征，将局部特征抽取的算法融入到了神经网络中

表情分析工具：FACS
