---
layout: post
title: "Python3网络爬虫开发实战（2）"
author: "yuruj"
catelog: true
header-style: text
tags:
  - 爬虫
  - Python
  - 学习笔记
---

# 第四章 解析库的使用

## 使用XPath

P158

## 使用Beautiful Soup

P168

## 使用pyquery

### 初始化

#### 字符串初始化

```python
from pyquery import PyQuery as pq
doc = pq(html)
print(doc('li'))

<li class='...'>....</li>
<li class='...'>....</li>
<li class='...'>....</li>
```

#### URL初始化

```python
from pyquery import PyQuery as pq
doc = pq(url='.....')
print(doc('title'))
#这两者等价
from pyquery import PyQuery as pq
import requests
doc = pq(requests.get('.....').text)
```

#### 文件初始化

```python
from pyquery import PyQuery as pq
doc = pq(filename='demo.html')
print(doc('li'))
```

### 基本CSS选择器

```python
from pyquery import PyQuery as pq
doc = pq(html)
print(doc('#container .list li'))
print(type(doc('#container .list li')))

<class 'pyquery.pyquery.PyQuery'>
```

### 查找节点

#### 字节点

```python
from pyquery import PyQuery as pq
doc = pq(html)
items = doc('.list')
print(type(items))
print(items)
lis = items.find('li')
print(type(lis))
print(lis)
```

find方法的查找范围是节点的所有子孙节点，结果的类型是PyQuery类型

如果我们只想查找子节点，那么可以用children方法

#### 父节点

我们可以用parent方法获取某个节点的父节点

如果想获取某个祖先节点，可以使用parents方法，也可以向该方法传入CSS选择器以筛选祖先节点

#### 兄弟节点

siblings方法

同样，可传入CSS选择器

### 遍历

pyquery的选择结果可能是多个节点，也可能是单个节点，类型都是PyQuery类型，并没有返回像Beautiful Soup那样的列表

对于单个节点，可以直接打印输出，也可以直接转成字符串

对于多个节点的结果，我们就需要遍历来获取了

```python
from pyquery import PyQuery as pq
doc = pq(html)
lis.doc('li').items()
print(type(lis))
for li in lis:
  print(li,type(li))
  
<class 'generator'>
<li class='item-0'>first item</li>
<class 'pyquery.pyquery.PyQuery'>
```

调用items方法后，会得到一个生成器，遍历一下，就可以逐个得到li节点对象了，它的类型也是PyQuery类型

每个li节点还可以调用前面所说的方法进行选择，比如继续查询字节点，非常灵活

### 获取信息

#### 获取属性

提取到某个PyQuery类型的节点后，就可以调用attr方法来获取属性

```python
from pyquery import PyQuery as pq
doc = pq(html)
a = doc('.item-0.active a')
print(a, type(a))
print(a.attr('href'))
#这两者结果完全一样
#print(a.attr.href)

<a href='link3.html'><span class='bold'>third item</span></a>
<class 'pyquery.pyquery.PyQuery'>
	link3.html
```

当返回结果是包含多个节点时，调用attr方法，只会得到第一个节点的属性

要想获取所有节点的属性，可以遍历

#### 获取文本

text方法可以获取其内部的文本信息，忽略掉节点内部包含的所有HTML，只返回纯文字内容

```python
from pyquery import PyQuery as pq
doc = pq(html)
a = doc('.item-0.active a')
print(a)
print(a.text())

<a href='link3.html'><span class='bold'>third item</span></a>
third item
```

但如果想获取这个节点内部的HTML文本，就要用html方法

```python
from pyquery import PyQuery as pq
doc = pq(html)
li = doc('.item-0.active')
print(li.html())

<a href='link3.html'><span class='bold'>third item</span></a>
```

如果得到的结果是多个节点，并且想要获取每个节点的内部HTML文本，则需要遍历每个节点。而text方法不需要遍历就可以获得，它将所有节点取文本之后合并成一个字符串，中间用一个空格分隔开

### 节点操作

#### addClass和removeClass

动态改变节点的class属性

```python
from pyquery import PyQuery as pq
doc = pq(html)
li = doc('.item-a.active')
print(li)
li.removeClass('active')
print(li)
li.addClass('active')
print(li)

<li class='item-0 active'><a href='link3.html'><span class='bold'>third item</span></a></li>
<li class='item-0'><a href='link3.html'><span class='bold'>third item</span></a></li>
<li class='item-0 active'><a href='link3.html'><span class='bold'>third item</span></a></li>
```

#### att、text和html

如果attr方法只传入第一个参数的属性名，则是获取这个属性值，如果传入第二个参数，可以用来修改属性值

text和html方法如果不传参数，则是获取节点内的纯文本和HTML文本，如果传入参数，则进行赋值

#### remove()

```python
from pyquery import PyQuery as pq
doc = pq(html)
wrap = doc('.wrap')
print(wrap.text())
wrap.find('p').remove()
print(wrap.text())
```

### 伪类选择器

CSS选择器之所以强大，还有一个很重要的原因，那就是它支持各种各样的伪类选择器，例如选择第一个节点、最后一个节点、奇偶数节点、包含某一文本的节点等

# 第五章数据存储

## 文件存储

### TXT文本存储

将数据保存到TXT文本的操作非常简单，而且TXT文本几乎兼容任何平台，但是不利于检索

如果对检索和数据结构要求不高，追求方便第一的话，可以采用TXT文本存储

```python
import requests
from query import PyQuery as pq

url = 'https://www.zhihu.com/explore'
headers = {
  'User-Agent': '....'
}
html = requests.get(url, headers = headers).text
doc = pq(html)
items = doc('.explore-tab .feed-item').item()
for item in items:
  question = item.find('h2').text()
  author = item.find('.author-link-line').text()
  answer = pq(item.find('.content').html()).text()
  file = open('explore.txt', 'a', encoding='utf-8')
  file.write('\n'.join([question, author, answer]))
  file.write('\n' + '=' * 50 + '\n')
  file.close()
```

这样热门回答的内容就被保存成文本形式

open方法的第一个参数即要保存的目标文件名称，第二个参数为a，代表以追加方式写入到文本。另外，我们还指定了文件的编码为utf-8。最后，写入完成后，还需要调用close方法来关闭文件对象

#### 打开方式

r：只读，默认模式，文件的指针将会放在文件的开头

rb：以二进制只读方式打开一个文件，文件的指针将会放在文件的开头

r+：以读写方式打开一个文件，文件的指针将会放在文件的开头

rb+：以二进制读写方式打开一个文件，文件的指针将会放在文件的开头

w：以写入方式打开一个文件。如果该文件已存在，则将其覆盖。如果该文件不存在，则创建新文件

wb：以二进制写入方式打开一个文件。如果该文件已存在，则将其覆盖。如果该文件不存在，则创建新文件

w+：以读写方式打开一个文件。如果该文件已存在，则将其覆盖。如果该文件不存在，则创建新文件

wb+：以二进制读写格式打开一个文件。如果该文件已存在，则将其覆盖。如果该文件不存在，则创建新文件

a：以追加方式打开一个文件。如果该文件已存在，文件指针将会被放在文件结尾。如果该文件不存在，则创建新文件来写入

ab：以二进制追加方式打开一个文件。如果该文件已存在，文件指针将会被放在文件结尾。如果该文件不存在，则创建新文件来写入

a+：以读写方式打开一个文件。如果该文件已存在，文件指针将会被放在文件结尾。如果该文件不存在，则创建新文件来读写

ab+：以二进制追加方式打开一个文件。如果该文件已存在，文件指针将会被放在文件结尾。如果该文件不存在，则创建新文件来读写

#### 简化写法

with as语法，在with控制块结束时，文件会自动关闭，所以就不需要再调用close方法

```python
with open('explore.txt', 'a', encoding='utf-8') as file:
  file.write('\n'.join([question, author, answer]))
  file.write('\n' + '=' * 50 + '\n')
```

这种方法简单易用，操作高效，是一种最基本保存数据的方法

### JSON文件存储

JSON，全程为JavaScript Object Notation，也就是JS对象标记，它通过对象和数组的组合来表示数据，构造简单但结构化程度非常高，是一种轻量级的数据交换格式

#### 对象和数组

在JS语言中，一切都是对象。因此，任何支持的类型都可以通过JSON来表示，例如字符串、数字、对象、数组等，但是对象和数组是比较特殊且常用的两种类型

对象：

它在JS中时使用花括号{}包裹起来的内容，数据结构为{key1: value1, key2: value2, ...}的键值对结构。在面向对象的语言中，key为对象的属性，value为对应的值。键名可以用整数和字符串来表示，值的类型可以是任意类型

数组：

数组在JS中是方括号[]包裹起来的内容，数据结构为['java', 'js', ...]的索引结构，在JS中，数组是一种比较特殊的数据结构，它可以像对象那样使用键值对，但还是索引用的多。同样，值的类型可以是任意类型

```python
[{
    "name": "Bob",
    "gender": "male"
},{
    "name": "Selina",
  	"gender": "female",
}]
```

有中括号包围的就相当于列表类型，列表中的每个元素可以是任意类型，这个示例中它是字典类型，由大括号包围

JSON可以由以上两种形式自由组合而成，可以无限次嵌套，结构清晰，是数据交换的极佳方式

#### 读取JSON

调用JSON库的loads方法将JSON文本字符串转为JSON对象，可以通过dumps方法将JSON对象转为文本字符串

```python
import json

str = '''
....
'''
print(type(str))
data = json.loads(str)
print(data)
print(type(data))
<class 'str'>
.....
<class 'list'>
```

由于最外层是中括号，所以最终的类型是列表类型

```python
data[0]['name']
data[0].get('name')
```

推荐使用get方法，因为此时如果键名不存在，则不会报错，返回None，另外，get方法可以传入第二个参数即默认值，在不存在的情况下不返回None，返回默认值

JSON的数据需要用双引号包围，而不能用单引号,否则会解析失败

```python
import json

with open('data.json', 'r') as file:
  str = file.read()
  data = json.loads(str)
  print(data)
```

#### 输出JSON

```python
improt json

data = [{
  'name': 'Bob',
  'gender': 'male'
}]
with open('data.json', 'w') as file:
  file.write(json.dumps(data))
```

另外，想保存JSON格式，可以再加一个参数indent，代表缩进字符个数

```python
with open('data.json', 'w') as file:
  file.write(json.dumps(data, indent=2))
```

这样得到的内容会自动带缩进，格式更加清晰

若想输出的JSON中包含中文字符，而不是将值输出后变成Unicode码

```python
with open('data.json', 'w', encoding='utf-8') as file:
  file.write(json.dumps(data, indent=2, ensure_ascii=False))
```

### CSV文件存储

CSV，全程为Comma-Seperated Values，中文可以叫做逗号分隔值或字符分隔值，其文件以纯文本形式存储表格数据

比Excel文件更加简洁，XLS文本是电子表格，它包含了文本、数值、公式和格式等内容，而CSV中不包含这些内容，就是特定字符分隔的纯文本，结构简单清晰

该文件是一个字符序列，可以由任意树木的记录组成，记录间以某种换行符分隔。每条记录由字段组成，字段间的分隔符是其他字符或字符串，最常见的是逗号或制表符。

所有记录都有完全相同的字段序列，相当于一个结构化表的纯文本形式

#### 写入

P204

#### 读取

P206

## 关系型数据库存储

关系型数据库是基于关系模型的数据库，而关系模型是通过二维表来存储的。所以它的存储方式就是行列组成的表。

表可以看作某个实体的集合，而实体之间存在联系，这就需要表与表之间的关联体系来体现。多个表组成一个数据库，也就是关系型数据库。

P207

## 非关系型数据库存储

NoSQL，not only SQL

基于键值对，不需要经过SQL层的解析，数据之间没有耦合，性能非常高

键值存储数据库 列存储数据库 文档型数据库 图形数据库

对于爬虫的数据存储来说，一条数据可能存在某些字段提取失败而缺失的情况，而且数据可能随时调整。另外，数据之间可能还存在数据嵌套关系。如果使用关系型数据库，一是需要提前建表，二是如果存在数据嵌套关系的话，需要进行序列化操作才可以存储，这非常不方便。如果使用了非关系型数据库，就可以避免一些麻烦，更简单高效

P213

# 第六章 Ajax数据爬取

有时候我们在用requests抓取页面的时候，得到的结果可能和浏览器中看到的不一样：在浏览器中可以看到正常的页面数据，但是使用requests得到的结果并没有，这是因为requests获取的都是原始的HTML文档，而浏览器中的页面则是经过JS处理数据后生成的结果，这些数据的来源有多种，可能是通过Ajax加载的，可能是包含在HTML文档，也可能是经过JS和特定算法计算后生成的

对于对一种情形，数据加载是一种异步加载方式。原始的页面最初不会包含某些数据，原始页面加载完后，会再向服务器请求某个接口获取数据，然后数据才会被处理从而呈现到网页上，这其实就是发送了一个Ajax请求

照Web发展的趋势来看，这种形式的页面越来越多。网页的原始HTML文档不会包含任何数据，数据都是通过Ajax统一加载后再呈现出来的，这样在Web开发上可以实现前后端分离，而且降低服务器直接渲染页面带来的压力

所以如果遇到这样的页面，直接利用requests等库来抓取原始页面，是无法获取到有效数据的，这时需要分析网页后台向接口发送的Ajax请求，如果可以用requests来模拟Ajax请求，那么就可以成功抓去了

## 什么是Ajax

Asynchronous JavaScript XML 即异步的XML

它不是一门编程语言，而是利用JavaScript在保证页面不被刷新、页面链接不改变的情况下与服务器交换数据并更新部分网页的技术

对于传统的网页，如果想更新其内容，那么就必须要刷新整个页面，但有了Ajax，便可以在页面不被全部刷新的情况下更新其内容。在这个过程中，页面实际上是在后台与服务器进行了数据交互，获取到数据之后，再利用JS改变网页，这样网页内容就会更新了

### 实例引入

很多网页下滑查看更多，出现一个加载动画，这其实就是Ajax加载的过程

### 基本原理

JS向服务器发送了一个Ajax请求，然后获取新数据，将其解析，并将其渲染在网页中

## Ajax分析方法

### 查看请求

Network组件

Ajax有其特殊的请求类型，叫做xhr

点击preview，即可看到响应的内容，它是JSON格式的。这里Chrome为我们自动做了解析

另外，也可以切换到Response选项卡，从中观察到真实的返回数据

### 过滤请求

在请求的上方有一层筛选栏，直接点击XHR，此时在下方显示的所有请求便都是Ajax请求了

## Ajax结果提取

### 分析请求

以微博为例，可以发现，这一个GET类型的请求，请求连接有4个参数：type，value，containerid，page

type始终为uid，value的值就是页面链接中的数字，其实这就是用户的id。另外，还有containerid。可以发现，他就是107603加上用户id。改变的值就是page，很明显这个参数是用来控制分页的，page=1代表第一页

### 分析响应

```python
from urllib.parse import urlencode
import requests
base_url = 'https://m.weibo.cn/api/container/getIndex?'

headers = {
  'Host': 'm.weibo.cn',
  'Referer': 'https://m.weibo.cn/u/2830678474',
  'User-Agent': '....',
  'X-Requested-With': 'XMLHttpRequest',
}

def get_page(page):
  params = {
    'type': 'uid',
    'value': '...',
    'containerid': '....'
    'page': page
  }
  url = base_url + urlencode(params)
  try:
    response = requests.get(url, headers=headers)
    if response.status_code == 200:
      retuen response.json()
	except requests.ConnectionError as e:
    print('Error', e.args)

from pyquery import PyQuery as pq

def parse_page(json):
  if json:
    items = json.get('data').get('cards'):
      for item in items:
        item = item.get('mblog')
        weibo = {}
        weibo['id'] = item.get('id')
        weibo['text'] = pq(item.get('text').text())
        weibo['attitudes'] = item.get('attitudes_count')
        weibo['comments'] = item.get('comments_count')
        weibo['reposts'] = item.get('reposts_count')
        yield weibo

if _name_ == '__main__':
  for page in range(1,11):
    json = get_page(page)
    results = parse_page(json)
    for result in results:
      print(result)
      
from pymongo import MongoClient

client = MongoClient()
db = client['weibo']
collection = db['weibo']

def save_to_mongo(result):
  if collection.insert(reuslt):
    print('Save to Mongo')
```

## 分析Ajax爬取今日头条街拍美图

P242

图片的名称可以使用其内容的MD5值，去除重复

# 第七章 动态页面爬取

Ajax其实是JS动态渲染页面的一种情形。不过JS动态渲染的页面不止是Ajax这一种。即使是Ajax获取的数据，但其接口含有很多加密参数，我们难以直接找出其规律，也很难直接分析Ajax来抓取

为了解决这些问题，我们可以直接使用模拟浏览器运行的方式来实现，这样就可以做到在浏览器中看到是什么样的，抓取的源码就是什么样的，也就是可见可爬。这样我们就不用再去管网页内部的JS用了什么算法渲染页面，也不用管网页后台的Ajax接口到底有哪些参数

## Selenium的使用

Selenium是一个自动化测试工具，利用它可以驱动浏览器执行特定的动作，如点击、下拉等操作，同时还可以获取浏览器当前呈现的页面的源代码，做到可见即可爬。对于一些JavaScript动态渲染的页面来说，此种抓取方式非常有效。

```python
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.comman.keys import Keys
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.support.wait import WebDriverWait

browser = webdriver.Chrome()
try:
	browser.get ('https://www.baidu.com')
	input = browser.find_element_by_id ('kw')
	input.send_keys('Python')
	input.send_keys(Keys.ENTER)
	wait = WebDriverWait(browser, 10)
	wait.until(EC.presence_of_element_located((By.ID,'content left'))) 			
  print(browser.current_url)
	print(browser.get_cookies())
	print(browser.page_source)
finally:
	browser.close()
```

运行代码后发现，会自动弹出一个Chrome浏览器。浏览器首先会跳转到百度，然后在搜索框中输入Python，接着跳转到搜索结果页。搜索结果加载出来后，控制台分别会输出当前的URL、当前的Cookies和网页源代码

用Selenium来驱动浏览器加载网页的话，就可以直接拿到JS渲染的结果了，不用担心使用的是什么加密系统

### 声明浏览器对象

```python
from selenium import webdriver

browser = webdriver.Chrome()
browser = webdriver.Firefox()
browser = webdriver.Edge()
browser = webdriver.PhantomJS()
browser= webdriver.Safari()
```

### 访问页面

```python
from selenium import webdnver
browser = webdriver.Chrome()
browser.get('https://www.taobao.com')
print(browser.page_source)
browser.close()
```

我们可以用get方法来请求网页，参数传入链接URL即可

### 查找节点

Selenium可以驱动浏览器完成各种操作，比如填充表格、模拟点击等。

#### 单个节点

P251

返回的是WebElement类型

#### 多个节点

P253

返回的是WebElement类型

### 节点交互

Selenium可以驱动浏览器来执行一些操作，也就是说可以让浏览器模拟执行一些动作。

比较常见的用法是：输入文字时用send_keys方法，清空文字时用clear方法，点击按钮时用click方法

### 动作链

在上面的实例中，一些交互动作都是针对某个节点执行的。另外还有一些动作，他们没有特定的执行对象，比如鼠标拖曳、键盘按键等，这些动作用另一种方式来执行，那就是动作链

### 执行JavaScript

对于某些操作，Selenium API并没有提供。比如，下拉进度条，它可以直接模拟运行JavaScript，此时可以使用execute_script方法即可实现

有了这个方法，基本上API没有提供的所有功能都可以用执行JavaScript的方式来实现了

### 获取节点信息

通过page_source属性可以获取网页的源代码，接着就可以使用解析库来提取信息了

不过，既然Selenium已经提供了选择节点的方法，返回的是WebElement类型，那么他也有相关的方法和属性来直接提取节点信息，这样的话，我们就可以不用通过解析源代码来提取信息，非常方便

#### 获取属性

get_attribute放啊放

#### 获取文本值

每个WebElement节点都有text属性，直接调用这个属性就可以得到节点内部的文本信息

### 获取id、位置、标签名和大小

WebElement节点还有其他属性，比如id属性可以获取节点id，location属性可以获取该节点在页面中的相对位置，tag_name属性可以获取标签名称，Size属性可以获取节点的大小，也就是宽高

### 切换Frame

网页中有一种节点叫做iframe，也就是子Frame，相当于页面中的子页面，它的结构和外部网页结构完全一致。Selenium打卡网页后，它默认是在父级Frame里面操作，而此时如果页面中还有子Frame，它是不能获取到子Frame里面的节点的。这时就需要使用switch_to.frame方法来切换Frame

### 延时等待

在Selenium中，get方法会在网页框架加载结束后结束执行，此时如果获取page_sorce,可能并不是浏览器完全加载完成的页面，如果某些页面有额外的Ajax请求，我们在网页源代码中也不一定能成功获取到。所以，这里需要延时等待一定时间，确保节点已经加载出来

#### 隐式等待

当使用隐式等待执行测试的时候，如果Selenium没有在DOM中找到节点，将继续等待，超过设定时间后，则抛出找不到节点的异常。换句话说，当查找节点而节点并没有立即出现的时候，隐式等待将等待一段时间后再查找DOM，默认的时间是0

```python
bowser.implicitly_wait(10)
```

#### 显式等待

隐式等待的效果其实并没有那么好，因为我们只规定了一个固定时间，而页面的加载时间会受到网络条件的影响。这里还有一种更合适的显式等待方法，它指定要查找的节点；如果到了规定时间依然没有加载出该节点，则抛出超时异常

P258

### 前进和后退

平常使用浏览器都有前进和后退功能，Selenium也可以完成这个操作，它使用back方法后退，使用forward方法前进

### Cookies

使用Selenium，还可以方便地对Cookies进行操作，例如获取、添加、删除Cookies等

### 选项卡管理

P261

### 异常处理

P261

## Splash的使用

P262

## Spalsh负载均衡配置

P286

## 使用Selenium爬取淘宝商品

可看即可爬

在开始之前，请确保已经正确安装好Chrome浏览器并配置好了ChromeDriver；另外，还需正确安装Python的Selenium库；最后，还对接了PhantomJS和Firefox，请确保安装好了PhantomJS和Firefox并配置好了GeckoDriver

### Chrome Headless模式

从Chrome59版本开始，已经开始支持Headless模式，也就是无界面模式，这样爬取的时候就不会弹出浏览器了

启用Headless模式的方式如下：

```python
chrome_options = webdriver.ChromeOptions()
chrome_options.add_argument('--headless')
browser = webdriver.Chrome(chrome_options=chrome_options)
```

首先，创建ChromeOptions对象是，接着添加headless参数，然后在初始化Chrome对象的时候通过chrome_options传递这个ChromeOptions对象，这样我们就可以成功呢启用Chrome的Headless模式了

### 对接Firefox

只需

```python
browser = webdriver.Firefox()
```

### 对接PhantomJS

PhatomJS是一个无界面浏览器

```python
browser = webdriver.PhantomJS()
```

另外，它还支持命令行配置。比如，可以设置缓存和禁用图片加载的功能，进一步提高爬取效率

# 第八章 验证码的识别

## 图形验证码的识别

P298

## 极验滑动验证码的识别

P301

## 点触验证码的识别

P311

## 微博宫格验证码的识别

P318

# 第九章 代理的使用

127.0.0.1

127.0.0.1:1087

## 代理的设置

免费代理大多数情况下都是不好用的，所以比较靠谱的方法是购买付费代理

如果本纪有相关代理软件的话，软件一般会在本机创建HTTP或SOCKS代理服务，本机直接使用此代理也可以

### urllib

P327

当请求的链接是http协议的时候，ProxyHandler会调用http代理，当请求的链接是https协议的时候，会调用https代理

如果遇到需要认证的代理，只需在代理前面加入代理认证的用户名密码即可。

```python
username:password@127.0.0.1:1087
```

如果代理是Socks5类型，可以用如下方式设置代理

```python
import socks
import socket
from urllib import request
from urllib.error import URLError

socks.set_default_proxy(socks.SOCKSS,’127.0.0.1’, 9742)
socket.socket = socks.socksocket
try:
	response = request.urlopen(’http://httpbin.org/get’)
	print(response.read().decode('utf-8'))
except URLError as e:
	print(e.reason)
```

### requests

对于requests来说，代理设置更加简单，我们只需要传入proxies参数即可

```python
import requests

proxy = {
  'http': '',
  'https': '',
}
try:
  response = requests.get('',proxies=proxies)
  print(response.text)
except requests.exceptions.ConnectionError as e:
  print('Error', e.args)

#SOCKS5代理(1) 需pip3 install 'requests[socks]'
import requests
proxy = 
proxies = {
  'http': 'socks5://' + proxy
  'https': 'socks5://' + proxy
}
try:
  response = requests.get('',proxies=proxies)
  print(response.text)
except requests.exceptions.ConnectionError as e:
  print('Error', e.args)
  
#SOCKS5代理(2) 此方法是全局设置
import socks
import socket
import requests

socks.set_default_proxy(socks.SOCKSS,’127.0.0.1’, 9742)
socket.socket = socks.socksocket
try:
  response = requests.get('')
  print(response.text)
except requests.exceptions.ConnectionError as e:
  print('Error', e.args)
```

### Selenium

#### Chrome

```python
from selenium import webdriver

proxy= '127.0.0.1:9743'
chrome_options = webdriver.ChromeOptions()
chrome_options.add_argument('--proxy-server=http://' + proxy)
browser = webdriver.Chrome(chrome_options=chrome_options)
browser.get('http://httpbin.org/get')
```

如果代理是认证代理，则设置方法相对比较麻烦，这需要在本地创建一个manifest.json配置文件和background.js脚本来设置认证代理。运行代码后本地会生成一个proxy_auth_plugin.zip文件来保存当前配置

P331

#### PhantomJS 

P332

## 代理池的维护

### 准备工作

首先要成功安装Redis数据库并启动服务，另外还需安装aiohttp、requests、redis-py、pyquery、Flask库

### 代理池的目标

基本模块：存储模块 获取模块 检测模块 接口模块 调度模块

Web API

P335

## 付费代理的使用

付费代理分为两类：
一类提供接口获取海量代理，按天或者按量收费，如讯代理

一类搭建了代理隧道，直接设置固定域名代理，如阿布云代理

P347

## ADSL拨号代理

代理池可以挑选出许多可用代理，但是常常其稳定性不高、响应速度慢，而且这些代理通常是公共代理，IP被封的概率很大。另外，这些代理可能有效时间比较短，虽然代理池一直在筛选，但如果没有及时更新状态，也有可能获取到不可用的代理

通过ASDL拨号代理，我们可以无限次更换IP，而且线路非常稳定，抓取效果好很多

P351

## 使用代理爬取微信公众号文章

如果服务器返回状态码为302而非200，就是IP访问次数太高，IP被封禁

实现请求的数据结构，构造队列

P364

# 第十章 模拟登陆

## 模拟登陆并爬取Github

### 分析登陆过程

打开开发者工具，将Preserve Log选项勾上，这表示显示持续日志

点击sessin请求，可以看到请求的页面是https://github.com/session，请求方式为POST

Form Data包含了5个字段，commit是固定的字符串Sign in，utf-8是一个勾选字符，authenticity_token较长，初步判断是一个Base64加密的字符串，login是登陆的用户名，password是登陆的密码

综上所述，我们现在无法直接构造的内容有Cookies和authenticity_token

在登陆之前我们会访问到一个登陆页面，此页面是通过GET形式访问的。输入用户名密码，点击登录按钮，浏览器发送这两部分信息，也就是说Cookies和authenticity_token一定是在访问登陆页的时候设置的

这是再退出登录，回到登陆页，同时清空Cookies，重新访问登陆页，截获发生的请求。Response Headers上有一个Set-Cookie字段。这就是设置Cookies的过程

另外，我们发现Response Headers没有和authenticity_token相关的信息，所以可能authenticity_token还隐藏在其他的页面或者是计算出来的

我们再从网页的源码探寻，搜索相关字段，发现源代码里面还隐藏着此信息，它是一个隐藏式表单元素

## Cookies池的搭建

很多时候，在爬取没有登陆的情况下，我们也可以访问一部分页面或请求一些接口，因为毕竟网站本身需要做SEO，不会对所有页面都设置登陆限制

但是，不登陆直接爬取会有一些弊端，弊端主要有以下两点：

设置了登陆限制的页面无法爬取

一些页面或接口虽然可以直接请求，但是请求一旦频繁，访问就容易被限制或者IP直接被封，但是登陆以后就不会出现这样的问题，因此登陆之后被反爬的可能性更低

登陆账号可以降低被封禁的概率

我们可以尝试登陆以后再做爬取，被封禁的几率会小很多

如果需要做大规模抓取，我们就需要拥有很多账号，每次请求随机选取一个账号，这样就降低了单个账号的访问频率，被封的概率又会大大降低

维护多个账号的登陆信息就需要用到Cookies池

自动生成Cookies、定时监测Cookies、提供随机Cookies

存储模块 获取模块 检测模块 接口模块 调度模块

P385

# 第十一章 App的爬取

App的爬取相比Web端爬取更加容易，反爬虫能力没有那么强，而且数据大多是以JSON形式传输的，解析更加简单

在Web端，我们可以通过浏览器的开发者工具监听到各个网络请求和响应过程，在App端如果想要查看这些内容就需要借助抓包软件

我们可以通过设置代理的方式将手机处于抓包软件的监听之下，这样便可以看到App在运行过程中发生的所有请求和响应了，相当于分析Ajax一样。如果这些请求的URL、参数等都是有规律的，那么总结出规律直接用程序模拟爬取即可，如果他们没有规律，我们可以利用另一个工具mitmdump对接Python脚本直接处理Response。另外，App的爬取肯定不能由人来完成，也需要做到自动化，所以我们还要对App进行自动化控制，这里利用到的库是Appium

## Charles的使用

Charles是一个网络抓包工具，我们可以用它来做App的抓包分析，得到App运行过程中发生的所有网络请求和响应内容，这就和Web端浏览器的开发者工具Network部分看到的结果一致

### 准备工作

确保已经正确安装Charles并开启了代理服务，手机和Charles处于同一个局域网下，Charles代理和CharlesCA证书设置好

### 原理

首先Charles运行在自己的PC上，Charles运行的时候会在PC的8888端口开启一个代理服务，这个服务实际上是一个HTTP/HTTPS的代理

确保手机和PC在同一个局域网内，我们可以使用手机模拟器通过虚拟网络连接，也可以使用手机真机和PC通过无线网络连接

设置手机代理为Charlse的代理地址，这样手机访问互联网的数据包就会流经Charles，Charles再转发这些数据到真实的服务器，服务器返回的数据包再由Charles转发回手机，Charles就起到中间人的作用，所有流量包都可以捕捉到，因此所有HTTP请求和响应都可以捕获到。同时Charles还有权利对请求和响应进行修改

P398

## mitmproxy的使用

P405

## mitmdump爬取“得到”APP电子书信息

P417

## Appium的基本使用

P423

## Appium爬取微信朋友圈

P433

## Appium+mitmdump爬取京东商品

P437

