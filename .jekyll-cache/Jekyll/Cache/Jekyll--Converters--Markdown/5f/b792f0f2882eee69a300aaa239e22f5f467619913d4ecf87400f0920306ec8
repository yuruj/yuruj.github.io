I"<h1 id="第八章-深度学习">第八章 深度学习</h1>

<p>深度学习是加深了层的深度神经网络</p>

<p>基于之前介绍的网络，只需通过叠加层，就可以创建深度网络</p>

<h2 id="加深网络">加深网络</h2>

<p><img src="/img/typora-user-images/截屏2020-05-12 下午2.17.20.png" alt="" /></p>

<p>基于3 * 3的小型滤波器的卷积层</p>

<p>激活函数是ReLU</p>

<p>全连接层的后面使用Dropout层</p>

<p>基于Adam的最优化</p>

<p>使用He初始值作为权重初始值</p>

<p>这次的深度CNN尽管识别精度很高，但是对于某些图像，也犯了和人类同样的识别错误。从这一点上，我们也可以感受到深度CNN中蕴藏着巨大的可能性</p>

<h3 id="进一步提高识别精度">进一步提高识别精度</h3>

<p>对于MNIST数据集，层不同特别深就获得了目前最高的识别精度。一般认为，这是因为对于手写数字识别这样一个比较简单的任务，没有必要将网络的表现力提高到那么高的程度。因此，可以说加深层的好处并不大。而之后要介绍的大规模的一般物体识别的情况，因为问题复杂，所以加深层对提高识别精度大有裨益</p>

<p>集成学习 学习率衰减 Data Augmentation 数据扩充</p>

<p>Data Augmentation基于算法人为的扩充输入图像</p>

<p><img src="/img/typora-user-images/截屏2020-05-12 下午2.23.52.png" alt="" /></p>

<p>虽然这个看上去只是一个简单的技巧，不过经常会有很好的效果</p>

<h3 id="加深层的动机">加深层的动机</h3>

<p>加深层的网络可以减少网络的参数数量。说的详细一点，就是与没有加深层的网络相比，加深了层的网络可以用更少的参数达到同等水平（或者更强的）表现力</p>

<p><img src="/img/typora-user-images/截屏2020-05-12 下午2.38.05.png" alt="" /></p>

<p><img src="/img/typora-user-images/截屏2020-05-12 下午2.38.41.png" alt="" /></p>

<p>通过叠加卷积层，参数数量减小了。而且，这个参数数量之差会随着层的加深而变大</p>

<p>叠加小型滤波器来加深网络的好处可以减少参数的数量，扩大感受野（receptive field，给神经元施加变化的某个局部空间区域）。并且。通过叠加层，将ReLU等激活函数夹在卷积层的中间，进一步提高了网络的表现力。这是因为向网络添加了基于激活函数的非线性表现力，通过非线性函数的叠加，可以表现更加复杂的东西</p>

<p>加深层的另一个好处是使学习更加高效</p>

<p><img src="/img/typora-user-images/截屏2020-05-12 下午2.43.07.png" alt="" /></p>

<h2 id="深度学习的小历史">深度学习的小历史</h2>

<p><img src="/img/typora-user-images/截屏2020-05-12 下午2.44.32.png" alt="" /></p>

<h3 id="imagenet">ImageNet</h3>

<p><img src="/img/typora-user-images/截屏2020-05-12 下午2.45.53.png" alt="" /></p>

<h3 id="vgg">VGG</h3>

<p>VGG是由卷积层和池化层构成的基础的CNN。不过，它的特点是在于将有权重的层（卷积层或者全连接层）叠加至16层（或者19层），具备了深度（根据层的深度，有时也称VGG16或VGG19）</p>

<p><img src="/img/typora-user-images/截屏2020-05-12 下午2.48.31.png" alt="" /></p>

<p><img src="/img/typora-user-images/截屏2020-05-12 下午2.48.48.png" alt="" /></p>

<h3 id="googlenet">GoogLeNet</h3>

<p><img src="/img/typora-user-images/截屏2020-05-12 下午2.49.46.png" alt="" /></p>

<p>它基本上和之前介绍的CNN结构相同，不过GoogleNet的特征是，网络不仅在纵向上有深度，在横向上也有深度（广度）</p>

<p>GoogLeNet在横向上有宽度，这称为Inception结构</p>

<h3 id="resnet">ResNet</h3>

<p>微软团队开发的网络。它的特征在于具有比以前的网络更深的结构</p>

<p>P246</p>

<p><img src="/img/typora-user-images/截屏2020-05-12 下午2.53.34.png" alt="" /></p>

<p><img src="/img/typora-user-images/截屏2020-05-12 下午2.53.45.png" alt="" /></p>

<p><img src="/img/typora-user-images/截屏2020-05-12 下午2.53.58.png" alt="" /></p>

<h2 id="深度学习的高速化">深度学习的高速化</h2>

<p><img src="/img/typora-user-images/截屏2020-05-12 下午2.55.11.png" alt="" /></p>

<p>大部分时间都被耗费在卷积层上</p>

<p>如何高速、高效地进行卷积层中的运算是深度学习的一大课题</p>

<p>卷积层中进行的运算可以追溯至乘积累加运算</p>

<h3 id="基于gpu的高速化">基于GPU的高速化</h3>

<p><img src="/img/typora-user-images/截屏2020-05-12 下午3.18.28.png" alt="" /></p>

<p><img src="/img/typora-user-images/截屏2020-05-12 下午3.18.41.png" alt="" /></p>

<h3 id="分布式学习">分布式学习</h3>

<p><img src="/img/typora-user-images/截屏2020-05-12 下午3.19.51.png" alt="" /></p>

<p>Google的TensorFolw、微软的CNTK在开发过程中高度重视分布式学习。以大数据中心的低延迟、高吞吐网络作为支撑，基于这些框架的分布式学习呈现出惊人的效果</p>

<h3 id="运算精度的位数缩减">运算精度的位数缩减</h3>

<p>神经网络并不那么需要数值精度的位数。这是神经网络的一个重要性质</p>

<p>这个性质是基于神经网络的健壮性而产生的。这里所说的健壮性是指，比如，即便输入图像附有一些小的噪声，输出结果也仍然保持不变。可以认为，正是因为有了这个健壮性，流经网络的数据即便有所劣化，对输出结果的影响也较小</p>

<p><img src="/img/typora-user-images/截屏2020-05-12 下午3.25.05.png" alt="" /></p>

<p><img src="/img/typora-user-images/截屏2020-05-12 下午3.25.16.png" alt="" /></p>

<h2 id="深度学习的应用案例">深度学习的应用案例</h2>

<h3 id="物体检测">物体检测</h3>

<p>P253</p>

<h3 id="图像分割">图像分割</h3>

<p>P255</p>

<h3 id="图像标题的生成">图像标题的生成</h3>

<p>P256</p>

<h2 id="深度学习的未来">深度学习的未来</h2>

<h3 id="图像风格转换">图像风格转换</h3>

<p>P258</p>

<h3 id="图像的生成">图像的生成</h3>

<p>P259</p>

<h3 id="自动驾驶">自动驾驶</h3>

<p>P261</p>

<h3 id="deep-q-networl强化学习">Deep Q-Networl（强化学习）</h3>

<p>P262</p>
:ET