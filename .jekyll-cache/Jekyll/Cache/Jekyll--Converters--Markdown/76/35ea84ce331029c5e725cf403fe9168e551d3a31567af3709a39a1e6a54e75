I")<h1 id="第七章-卷积神经网络">第七章 卷积神经网络</h1>

<p>CNN被用于图像识别、语音识别等各种场合，在图像识别的比赛中，基于深度学习的方法几乎都以CNN为基础</p>

<h2 id="整体结构">整体结构</h2>

<p>CNN新出现了卷积层（Convolution层）和池化层（Pooling层）</p>

<p>在之前的神经网络中，相邻层的所有神经元之间都有连接，这称为全连接。我们用Affine层实现了全连接层</p>

<p><img src="/img/typora-user-images/截屏2020-05-12 上午8.56.57.png" alt="" /></p>

<p><img src="/img/typora-user-images/截屏2020-05-12 上午8.57.12.png" alt="" /></p>

<p>CNN的层的连接顺序是Convolution-ReLU-(Pooling)</p>

<p>Polling有时会被省略</p>

<p>还需要注意的是，CNN中靠近输出的层中使用了之前的Affine-ReLU组合。此外，最后的输出层中使用了之前的Affine-Softmax组合。这些都是一般的CNN中比较常见的结构</p>

<h2 id="卷积层">卷积层</h2>

<p>各层中传递的数据是有形状的数据</p>

<h3 id="全连接层存在的问题">全连接层存在的问题</h3>

<p>数据的形状被忽视了</p>

<p>图像是3维形状，这个形状中应该含有重要的空间信息。比如，空间上邻近的像素为相似的值、RGB的各个通道之间分别有密切的关联性、相距较远的像素没有什么关联等，3维形状中可能隐藏有值得提取的本质模式。但是，因为全连接层会忽视形状，将全部的输入数据作为相同的神经元（同一维度的神经元）处理，所以无法利用与形状相关信息</p>

<p>卷积层可以保持形状不变。当输入数据是图像时，卷积层会以3维数据的形式接收输入数据，并同样以3维数据的形式输出至下一层。因此，在CNN中，可以（有可能）正确理解图像等具有形状的数据</p>

<p>CNN中，有时将卷积层的输入输出数据称为特征图。其中，卷积层的输入数据称为输入特征图，输出数据称为输出特征图</p>

<h3 id="卷积运算">卷积运算</h3>

<p>卷积层进行的处理就是卷积运算。卷积运算相当于图像处理中的滤波器运算</p>

<p><img src="/img/typora-user-images/截屏2020-05-12 上午9.16.04.png" alt="" /></p>

<p>累积乘积运算</p>

<p>滤波器也有高长方向上的维度</p>

<p><img src="/img/typora-user-images/截屏2020-05-12 上午9.17.17.png" alt="" /></p>

<p>偏置通常只有一个（1*1）</p>

<h3 id="填充">填充</h3>

<p>使用填充主要为了调整输出的大小，卷积运算可以在保持空间大小不变的情况下将数据传给下一层</p>

<p>否则反复进行多次卷积运算的深度网络可能不断缩小最终无法再应用卷积运算</p>

<p><img src="/img/typora-user-images/截屏2020-05-12 上午9.20.13.png" alt="" /></p>

<h3 id="步幅">步幅</h3>

<p>应用滤波器的位置间隔称为步幅</p>

<p><img src="/img/typora-user-images/截屏2020-05-12 上午9.21.42.png" alt="" /></p>

<p>增大步幅后，输出大小会变小；而增大填充后，输出大小会变大</p>

<p><img src="/img/typora-user-images/截屏2020-05-12 上午9.22.55.png" alt="" /></p>

<p>当输出大小无法除尽时，需要采取报错等对策</p>

<p>根据深度学习框架的不同，当值无法除尽时，有时会向最接近的整数四舍五入，不进行报错而继续运行</p>

<h3 id="3维数据的卷积运算">3维数据的卷积运算</h3>

<p><img src="/img/typora-user-images/截屏2020-05-12 上午9.27.12.png" alt="" /></p>

<p>高、长、通道方向</p>

<p>滤波器大小可以设定为任意值，不过每个滤波器大小要全部相同</p>

<p>滤波器通道数只能设置为和输入数据的通道数相同的值</p>

<h3 id="结合方块思考">结合方块思考</h3>

<p><img src="/img/typora-user-images/截屏2020-05-12 上午9.31.49.png" alt="" /></p>

<p><img src="/img/typora-user-images/截屏2020-05-12 上午9.32.01.png" alt="" /></p>

<p><img src="/img/typora-user-images/截屏2020-05-12 上午9.32.26.png" alt="" /></p>

<p>滤波器的权重数据（20，3，5，5）</p>

<p>通道数为3，大小为5*5的滤波器有20个</p>

<h3 id="批处理">批处理</h3>

<p><img src="/img/typora-user-images/截屏2020-05-12 上午9.34.33.png" alt="" /></p>

<p>4维数据</p>

<p>批处理将N次的处理汇总成了1次处理</p>

<h2 id="池化层">池化层</h2>

<p>池化是缩小高、长方向上的空间的运算</p>

<p>缩小空间大小</p>

<p><img src="/img/typora-user-images/截屏2020-05-12 上午9.38.22.png" alt="" /></p>

<p>一般来说，池化的窗口大小会和步幅设定成相同的值</p>

<p>除了Max池化之外，还有Average池化等</p>

<p>池化层的特征</p>

<p>没有要学习的参数</p>

<p>通道数不发生改变</p>

<p>对微小的位置变化具有鲁棒性（健壮）</p>

<h2 id="卷积层和赤化层的实现">卷积层和赤化层的实现</h2>

<h3 id="四维数据">四维数据</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
</pre></td><td class="rouge-code"><pre><span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">28</span><span class="p">,</span> <span class="mi">28</span><span class="p">)</span> <span class="c1"># 随机生成数据
</span><span class="n">x</span><span class="p">.</span><span class="n">shape</span>
<span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">shape</span>
<span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">].</span><span class="n">shape</span>
<span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">].</span><span class="n">shape</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<h3 id="基于im2col的展开">基于im2col的展开</h3>

<p>如果老老实实地实现卷积运算，估计要重复好几层的for语句。这样的实现有点麻烦，而且，Numpy中存在使用for语句后处理变慢的缺点（Numpy中，访问元素时最好不用for语句）。这里，我们不使用for语句，而是使用im2col这个便利的函数进行简单的实现</p>

<p><img src="/img/typora-user-images/截屏2020-05-12 上午10.04.51.png" alt="" /></p>

<p><img src="/img/typora-user-images/截屏2020-05-12 上午10.05.19.png" alt="" /></p>

<p><img src="/img/typora-user-images/截屏2020-05-12 上午10.05.47.png" alt="" /></p>

<p><img src="/img/typora-user-images/截屏2020-05-12 上午10.06.07.png" alt="" /></p>

<h3 id="卷积层的实现">卷积层的实现</h3>

<p><img src="/img/typora-user-images/截屏2020-05-12 上午10.08.10.png" alt="" /></p>

<p>im2col会考虑滤波器大小、步幅、填充，将输入数据展开为2维数组</p>

<p>P219</p>

<h3 id="池化层的实现">池化层的实现</h3>

<p>池化层的实现和卷积层相同，都使用im2col展开输入数据</p>

<p>不过，池化的情况下，在通道方向上是独立的，这一点和卷积层不同</p>

<p>池化的应用区域按通道单独展开</p>

<p><img src="/img/typora-user-images/截屏2020-05-12 上午10.14.07.png" alt="" /></p>

<p><img src="/img/typora-user-images/截屏2020-05-12 上午10.14.21.png" alt="" /></p>

<p>P222</p>

<p>1、展开输入数据</p>

<p>2、求各行的最大值</p>

<p>3、转换为合适的输出大小</p>

<p>各阶段的实现都很简单，只有一两行代码</p>

<h2 id="cnn的实现">CNN的实现</h2>

<p>P224</p>

<p>如上所述，卷积层和池化层是图像识别中必备的模块</p>

<p>CNN可以有效读取图像中的某种特性，在手写数字识别中，还可以实现高精度的识别</p>

<h2 id="cnn的可视化">CNN的可视化</h2>

<h3 id="第一层权重的可视化">第一层权重的可视化</h3>

<p><img src="/img/typora-user-images/截屏2020-05-12 下午12.20.13.png" alt="" /></p>

<p><img src="/img/typora-user-images/截屏2020-05-12 下午12.20.41.png" alt="" /></p>

<p>卷积层的滤波器会提取边缘或斑块等原始信息</p>

<p>而刚才实现的CNN会将这些原始信息传递给后面的层</p>

<h3 id="基于分层结构的信息提取">基于分层结构的信息提取</h3>

<p>根据深度学习的可视化相关的研究，随着层数加深，提取的信息（正确地讲，是反映强烈的神经元）也越来越抽象</p>

<p><img src="/img/typora-user-images/截屏2020-05-12 下午12.24.57.png" alt="" /></p>

<p>如果堆积了多层卷积层，则随着层次加深，提取的信息也愈加复杂、抽象，这是深度学习中很有意思的一个地方。随着层次加深，神经元从简单的形状向高级信息变化。换句话来说，就像我们理解东西的含义一样，响应的对象在逐渐变化</p>

<h2 id="具有代表性的cnn">具有代表性的CNN</h2>

<h3 id="lenet">LeNet</h3>

<p>LeNet在1998年被提出，是进行手写数字识别的网络，他有连续的卷积层和池化层（正确地讲，是只抽选元素的子采样层），最后经全连接层输出结果</p>

<p><img src="/img/typora-user-images/截屏2020-05-12 下午12.31.18.png" alt="" /></p>

<p>LeNet中使用sigmoid函数，而现在的CNN中主要适用ReLU函数</p>

<p>原始的LeNet中使用子采样缩小中间数据的大小，而现在的CNN中Max池化是主流</p>

<h3 id="alexnet">AlexNet</h3>

<p>AlexNet是引发深度学习浪潮的导火线</p>

<p><img src="/img/typora-user-images/截屏2020-05-12 下午12.33.04.png" alt="" /></p>

<p>AlexNet叠有多个卷积层和池化层，最后经由全连接层输出结果</p>

<p>激活函数使用ReLU</p>

<p>使用进行局部正规化的LRN层</p>

<p>使用Dropout</p>

<p>关于网络结构，LeNet和AlexNet没有太大的不同。但是，围绕他们的环境和计算机技术有了很大的进步。具体地说，现在任何人都可以获得大量的数据。而且，擅长大规模并行计算的GPU得到普及，高速进行大量的运算已经成为可能。大数据和GPU已成为深度学习发展的巨大的原动力</p>

<p>GPU和大数据给这些课题带来了希望</p>
:ET