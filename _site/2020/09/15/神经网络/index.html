<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="google-site-verification" content="xBT4GhYoi5qRD5tr338pgPM5OWHHIDR6mNg1a3euekI" />
    <meta name="viewport" content="width=device-width, initial-scale=1, viewport-fit=cover">
    <meta name="description" content="">
    <meta name="keywords"  content="">
    <meta name="theme-color" content="#000000">
    
    <!-- Open Graph -->
    <meta property="og:title" content="深度学习入门：基于Python的理论与实现（1） - 余润杰的博客 | Yurj Blog">
    
    <meta property="og:type" content="article">
    <meta property="og:description" content="第一章 Python入门
">
    
    <meta property="article:published_time" content="2020-09-15T00:00:00Z">
    
    
    <meta property="article:author" content="yurj">
    
    
    <meta property="article:tag" content="AI">
    
    <meta property="article:tag" content="深度学习">
    
    <meta property="article:tag" content="Python">
    
    <meta property="article:tag" content="学习笔记">
    
    
    <meta property="og:image" content="http://localhost:4000/img/ybmq.jpg">
    <meta property="og:url" content="http://localhost:4000/2020/09/15/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/">
    <meta property="og:site_name" content="余润杰的博客 | Yurj Blog">
    
    <title>深度学习入门：基于Python的理论与实现（1） - 余润杰的博客 | Yurj Blog</title>

    <!-- Web App Manifest -->
    <link rel="manifest" href="/pwa/manifest.json">

    <!-- Favicon -->
    <link rel="shortcut icon" href="/img/favicon.ico">
    
    <!-- Canonical URL -->
    <link rel="canonical" href="http://localhost:4000/2020/09/15/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/">

    <!-- Bootstrap Core CSS -->
    <link rel="stylesheet" href="/css/bootstrap.min.css">

    <!-- Custom CSS -->
    <link rel="stylesheet" href="/css/hux-blog.min.css">

    <!-- Custom Fonts -->
    <!-- <link href="http://maxcdn.bootstrapcdn.com/font-awesome/4.3.0/css/font-awesome.min.css" rel="stylesheet" type="text/css"> -->
    <!-- Hux change font-awesome CDN to qiniu -->
    <link href="//cdnjs.cloudflare.com/ajax/libs/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet" type="text/css">


    <!-- HTML5 Shim and Respond.js IE8 support of HTML5 elements and media queries -->
    <!-- WARNING: Respond.js doesn't work if you view the page via file:// -->
    <!--[if lt IE 9]>
        <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
        <script src="https://oss.maxcdn.com/libs/respond.js/1.4.2/respond.min.js"></script>
    <![endif]-->

    <!-- ga & ba script hoook -->
    <script></script>
</head>


<!-- hack iOS CSS :active style -->
<body ontouchstart="">

    <!-- Navigation -->

<nav class="navbar navbar-default navbar-custom navbar-fixed-top invert">
    
        <div class="container-fluid">
            <!-- Brand and toggle get grouped for better mobile display -->
            <div class="navbar-header page-scroll">
                <button type="button" class="navbar-toggle">
                    <span class="sr-only">Toggle navigation</span>
                    <span class="icon-bar"></span>
                    <span class="icon-bar"></span>
                    <span class="icon-bar"></span>
                </button>
                <a class="navbar-brand" href="/">Yurj Blog</a>
            </div>

            <!-- Collect the nav links, forms, and other content for toggling -->
            <div id="huxblog_navbar">
                <div class="navbar-collapse">
                    <ul class="nav navbar-nav navbar-right">
                        <li>
                            <a href="/">Home</a>
                        </li>
                        
                        
                        
                        
                        <li>
                            <a href="/about/">About</a>
                        </li>
                        
                        
                        
                        <li>
                            <a href="/archive/">Archive</a>
                        </li>
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        <li class="search-icon">
                            <a href="javascript:void(0)">
                                <i class="fa fa-search"></i>
                            </a>
                        </li>
                    </ul>
                </div>
            </div>
            <!-- /.navbar-collapse -->
        </div>
        <!-- /.container -->
    </nav>

    <script>
        // Drop Bootstarp low-performance Navbar
        // Use customize navbar with high-quality material design animation
        // in high-perf jank-free CSS3 implementation
        var $body = document.body;
        var $toggle = document.querySelector('.navbar-toggle');
        var $navbar = document.querySelector('#huxblog_navbar');
        var $collapse = document.querySelector('.navbar-collapse');

        var __HuxNav__ = {
            close: function () {
                $navbar.className = " ";
                // wait until animation end.
                setTimeout(function () {
                    // prevent frequently toggle
                    if ($navbar.className.indexOf('in') < 0) {
                        $collapse.style.height = "0px"
                    }
                }, 400)
            },
            open: function () {
                $collapse.style.height = "auto"
                $navbar.className += " in";
            }
        }

        // Bind Event
        $toggle.addEventListener('click', function (e) {
            if ($navbar.className.indexOf('in') > 0) {
                __HuxNav__.close()
            } else {
                __HuxNav__.open()
            }
        })

        /**
         * Since Fastclick is used to delegate 'touchstart' globally
         * to hack 300ms delay in iOS by performing a fake 'click',
         * Using 'e.stopPropagation' to stop 'touchstart' event from 
         * $toggle/$collapse will break global delegation.
         * 
         * Instead, we use a 'e.target' filter to prevent handler
         * added to document close HuxNav.  
         *
         * Also, we use 'click' instead of 'touchstart' as compromise
         */
        document.addEventListener('click', function (e) {
            if (e.target == $toggle) return;
            if (e.target.className == 'icon-bar') return;
            __HuxNav__.close();
        })
    </script>
    <!-- Search -->
<div class="search-page">
  <div class="search-icon-close-container">
    <span class="search-icon-close">
      <i class="fa fa-chevron-down"></i>
    </span>
  </div>
  <div class="search-main container">
    <div class="row">
      <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
        <form></form>
        <input type="text" id="search-input" placeholder="$ grep...">
        </form>
        <div id="search-results" class="mini-post-list"></div>
      </div>
    </div>
  </div>
</div>

    <!-- Image to hack wechat -->
<!-- <img src="/img/icon_wechat.png" width="0" height="0"> -->
<!-- <img src="/img/timg.jpg" width="0" height="0"> -->

<!-- Post Header -->



<style type="text/css">
    header.intro-header{
        position: relative;
        background-image: url('/img/timg.jpg');
        background: ;
    }

    
</style>

<header class="intro-header style-text" >

    <div class="header-mask"></div>
    
    <div class="container">
        <div class="row">
            <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
                <div class="post-heading">
                    <div class="tags">
                        
                        <a class="tag" href="/archive/?tag=AI" title="AI">AI</a>
                        
                        <a class="tag" href="/archive/?tag=%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0" title="深度学习">深度学习</a>
                        
                        <a class="tag" href="/archive/?tag=Python" title="Python">Python</a>
                        
                        <a class="tag" href="/archive/?tag=%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0" title="学习笔记">学习笔记</a>
                        
                    </div>
                    <h1>深度学习入门：基于Python的理论与实现（1）</h1>
                    
                    <h2 class="subheading">神经网络</h2>
                    <span class="meta">Posted by yurj on September 15, 2020</span>
                </div>
            </div>
        </div>
    </div>
</header>






<!-- Post Content -->
<article>
    <div class="container">
        <div class="row">

    <!-- Post Container -->
            <div class="
                col-lg-8 col-lg-offset-2
                col-md-10 col-md-offset-1
                post-container">

                <!-- Multi-Lingual -->
                

				<h1 id="第一章-python入门">第一章 Python入门</h1>

<p>Python解释器能够以对话模式执行程序</p>

<p>.py即为Python脚本文件</p>

<p>类模板</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
</pre></td><td class="rouge-code"><pre><span class="k">class</span> <span class="nc">类名</span><span class="err">：</span>
	<span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">参数</span><span class="p">,</span> <span class="p">...):</span>
    <span class="p">...</span>
  <span class="k">def</span> <span class="nf">方法名1</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">参数</span><span class="p">,</span> <span class="p">...):</span>
		<span class="p">...</span>
  <span class="k">def</span> <span class="nf">方法名2</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">参数</span><span class="p">,</span> <span class="p">...):</span>
		<span class="p">...</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<p>在深度学习的实现中，经常出现数组和矩阵的计算。Numpy的数组类中提供了很多便捷的方法，在实现深度学习时，我们将使用这些方法。</p>

<p>在深度学习的实验中，图形的绘制和数据的可视化非常重要。Matplotlib是用于绘制图形的库，使用Matplotlib可以轻松的绘制图形和实现数据的可视化。</p>

<h1 id="第二章-感知机">第二章 感知机</h1>

<p>感知机是作为神经网络（深度学习）起源的算法</p>

<p>感知机的信号只有“流/不流”（1/0）两种取值</p>

<p>阈值：只有神经元计算出传送过来的信号的总和超过某一个界限值，才会输出1</p>

<p>权重：感知机的多个输入信号都有各自固有的权重，这些权重发挥着控制各个信号的重要性的作用。</p>

<h2 id="简单逻辑电路">简单逻辑电路</h2>

<p>通过改变感知机的参数（权重、阈值）即可表示与门、与非门、或门的逻辑电路</p>

<p>这里决定感知机参数的并不是计算机，而是我们人。我们看着真值表这种训练数据，人工考虑了参数的值。而机器学习的课题就是讲这个决定参数值的工作交由计算机自动进行。学习是确定合适的参数的过程，而人要做的是思考感知机的构造（模型），并把训练数据交给计算机</p>

<h2 id="偏置">偏置</h2>

<p>b + w1x1 + w2x2 &lt;= 0 –&gt; 0</p>

<p>b + w1x1 + w2x2 &gt; 0 –&gt; 1</p>

<p>b称为偏置，即-阈值</p>

<h2 id="感知机的局限性">感知机的局限性</h2>

<p>感知机无法实现异或门</p>

<p>感知机会生成由直线分割开的两个空间，其中一个空间输出1，另一个空间输出0</p>

<p>感知机的局限性在于它只能表示由一条直线分割的空间，即线性空间，像弯曲的曲线无法用感知机表示，即非线性空间</p>

<h2 id="多层感知机">多层感知机</h2>

<p>感知机的绝妙之处在于它可以叠加层</p>

<p>单层感知机无法分离非线性空间，但是通过组合感知机（叠加层）就可以实现异或门</p>

<p>异或门是一种多层结构的神经网络</p>

<p>实际上通过与非门的组合，就能再现计算机进行的处理</p>

<p>《计算机系统要素：从零开始构建现代计算机》</p>

<p>理论上可以说两层的感知机就能构建计算机。这是因为，已有研究证明，两层感知机（严格地来说是激活函数使用了非线性的sigmoid函数的感知机）可以表示任意函数</p>

<p>但是，使用两层感知机构造，通过设定合适的权重来构建计算机是一件非常累人的事情。实际上，在用与非门等低层的原件构建计算机的情况下，分阶段地制作所需的零件（模块）会比较自然，即先实现与门和或门，然后实现半加速器和全加速器，接着实现算数逻辑单元（ALU），然后实现CPU。因此，通过感知机表示计算机时，使用叠加了多层的构造来实现是比较自然的流程</p>

<h1 id="第三章-神经网络">第三章 神经网络</h1>

<p>神经网络的一个重要性质就是它可以自动的从数据中学习到合适的权重参数</p>

<h2 id="从感知机到神经网络">从感知机到神经网络</h2>

<p>输入层 输出层 中间层（隐藏层）</p>

<p>感知机</p>

<p>y = h(b + w1x1 + w2x2)</p>

<p>h(x) = 0      (x&lt;=0)</p>

<p>h(x) = 1      (x&gt;0)</p>

<p>h(x)激活函数</p>

<p>激活函数的作用在于决定如何来激活输入信号的总和</p>

<p>a = b + w1x1 + w2x2</p>

<p>y = h(a)</p>

<p>感知机的激活函数为阶跃函数，一旦输入超出阈值，就切换输出</p>

<p>如果将激活函数从阶跃函数换成其他函数，就可以进入神经网络的世界了</p>

<h2 id="激活函数">激活函数</h2>

<h3 id="sigmoid函数">sigmoid函数</h3>

<p>神经网络中经常使用的一个激活函数就是sigmoid函数</p>

<p>h(x) = 1/(1 + exp(-x))</p>

<p>实际上，上一章介绍的感知机和接下来要介绍的神经网络的主要区别就在于这个激活函数。其他方面，比如神经元的多层连接的结构、信号的传递方法等，基本上和感知机是一样的</p>

<h3 id="sigmoid函数和阶跃函数比较">sigmoid函数和阶跃函数比较</h3>

<p>平滑性</p>

<p>sigmoid函数是一条平滑的曲线，输入随着输出发生连续性的变化，</p>

<p>宏观上来看，两者具有相似的形状</p>

<p>两者均是输入小时，输出接近0；随着输入增大，输出向1靠近。也就是说，当输入信号为重要信息时，两者都会输出较大的值；当输入信号为不重要的信息时，两者都会输出较小的值</p>

<h3 id="非线性函数">非线性函数</h3>

<p>神经网络的激活函数必须使用非线性函数</p>

<p>如果使用线性函数的话，加深神经网络的层数就没有意义了</p>

<p>线性函数的问题在于，不过如何加深层数，总是存在与之等效的无隐藏层的神经网络</p>

<p>h(x) = cx</p>

<p>y(x) = h(h(h(x))) 对应3层神经网络 y(x) = c * c * c * x  ==&gt;  y(x) = c^3 * x</p>

<h3 id="relu函数">ReLU函数</h3>

<p>最近则主要使用ReLU函数</p>

<p>ReLU函数在输入大于0时，直接输出该值；在输入小于等于0时，输出0</p>

<h2 id="神经网络的内积">神经网络的内积</h2>

<p>通过矩阵的乘积一次性完成计算的技巧，在实现的层面上可以说是非常重要的</p>

<h2 id="3层神经网络的实现">3层神经网络的实现</h2>

<p>forward 前向</p>

<p><img src="/Users/yurunjie/Library/Application Support/typora-user-images/截屏2020-05-10 下午12.35.01.png" alt="截屏2020-05-10 下午12.35.01" style="zoom:50%;" /></p>

<p><img src="/Users/yurunjie/Library/Application Support/typora-user-images/截屏2020-05-10 下午12.35.46.png" alt="截屏2020-05-10 下午12.35.46" style="zoom:50%;" /></p>

<p><img src="/Users/yurunjie/Library/Application Support/typora-user-images/截屏2020-05-10 下午12.36.02.png" alt="截屏2020-05-10 下午12.36.02" style="zoom:50%;" /></p>

<h2 id="输出层的设计">输出层的设计</h2>

<p>神经网络可以用在分类问题和回归问题上，不过需要根据情况改变输出层的激活函数</p>

<p>一般而言，回归问题用恒等函数，分类问题用softmax函数</p>

<h3 id="恒等函数和softmax函数">恒等函数和softmax函数</h3>

<p><img src="/Users/yurunjie/Library/Application Support/typora-user-images/截屏2020-05-10 下午12.39.03.png" alt="截屏2020-05-10 下午12.39.03" style="zoom:50%;" /></p>

<p><img src="/Users/yurunjie/Library/Application Support/typora-user-images/截屏2020-05-10 下午12.40.07.png" alt="截屏2020-05-10 下午12.40.07" style="zoom:50%;" /></p>

<p><img src="/Users/yurunjie/Library/Application Support/typora-user-images/截屏2020-05-10 下午12.40.30.png" alt="截屏2020-05-10 下午12.40.30" style="zoom:50%;" /></p>

<p>输出层的各个神经元都受到所有输入信号的影响</p>

<h3 id="实现softmax函数时的注意事项">实现softmax函数时的注意事项</h3>

<p>溢出问题</p>

<p>某个指数函数的值很容易变得非常大，在这些超大值之间进行除法运算，结果会出现不确定的情况</p>

<p><img src="/Users/yurunjie/Library/Application Support/typora-user-images/截屏2020-05-10 下午12.43.31.png" alt="截屏2020-05-10 下午12.43.31" style="zoom:50%;" /></p>

<p>这里的C’可以使用任意值，但是为了防止溢出，一般会使用输入信号中的最大值</p>

<h3 id="softmax函数的特征">softmax函数的特征</h3>

<p>重要性质：softmax函数的输出值的总和是1</p>

<p>我们可以把softmax函数的输出结果解释为概率</p>

<p>这里需要注意的是，即便使用了softmax函数，各个元素的大小关系也不会改变。一般而言，神经网络只把输出值最大的神经元所对应的类别作为识别结果。并且，即便使用softmax函数，输出值最大的神经元的位置也不会变。因此，在实际的问题中，由于指数函数的运算需要一定的计算机运算量，因此输出层的softmax函数一般会被省略</p>

<p>求解机器学习问题的步骤可以分为：学习 + 推理</p>

<h3 id="输出层的神经元数量">输出层的神经元数量</h3>

<p>输出层的神经元数量需要根据待解决的问题来决定。对于分类问题，输出层的神经元数量一般设定为类别的数量</p>

<h2 id="手写数字识别">手写数字识别</h2>

<h3 id="mnist数据集">MNIST数据集</h3>

<p>这里使用的是MNIST手写数字图像集。MNIST是机器学习领域最有名的数据集之一，被应用于从简单的实验到发表的论文研究等各种场合</p>

<p>Python有pickle这个便利的功能。这个功能可以将程序运行中的对象保存为文件。如果加载保存过的pickle文件，可以立刻复原之前程序运行中的对象</p>

<h3 id="神经网络的推理处理">神经网络的推理处理</h3>

<p>输入层 784个神经元 输出层 10个神经元</p>

<p>2个隐藏层，第一个隐藏层有50个神经元，第二个隐藏层有100个神经元。这个50和100可以设置为任何值</p>

<p>预处理：对神经网络的输入数据进行某种既定的转换</p>

<p>正规化；把数据限定到某个范围内的处理</p>

<p>在这里，作为对输入图像的一种与处理，我们进行了正规化</p>

<h3 id="批处理">批处理</h3>

<p>输入数据的形状为100 * 784，输出数据的形状为100 * 10</p>

<p>这种打包式的输入数据称为批</p>

<p>批处理对与计算机的运算大有利处，可以大幅缩短每张图像的处理时间</p>

<p>因为大多数处理数值计算的库都进行了能够高效处理大型数组运算的最优化。并且，在神经网络的运算中，当数据传送成为瓶颈时，批处理可以减轻数据总线的负荷（严格来说，相对于数据读入，可以将更多的时间用在计算上）。也就是说，批处理一次性计算大型数组要比分开逐步计算各个小型数组速度更快</p>

<h1 id="第四章-神经网络的学习">第四章 神经网络的学习</h1>

<p>学习是指从训练数据中自动获取最优权重参数的过程</p>

<p>为了使神经网络能进行学习，将导入损失函数这一指标。学习的目标就是以该损失函数为基准，找出能使它的值达到最小的权重参数</p>

<h2 id="从数据中学习">从数据中学习</h2>

<p>在实际的神经网络中，参数数量成千上万，在层数更深的深度学习中，参数的数量甚至可以上亿</p>

<p>对于线性可分问题，感知机是可以利用数据自动学习的。根据感知机收敛定律，通过有限次数的学习，线性可分问题是可解的。但是，非线性可分问题则无法通过自动学习来解决</p>

<h3 id="数据驱动">数据驱动</h3>

<p>神经网络或深度学习则比以往的机器学习方法更能避免人为介入</p>

<p>与其绞尽脑汁，从零开始想出一个可以识别5的算法，不如考虑通过有效利用数据来解决这个问题。一种方案是，先从图形中提取特征量，再利用机器学习技术学习这些特征量的模式</p>

<p>这里所说的特征量是指可以从输入数据中准确地提取本质数据的转换器。图像的特征量通常表示为向量的形式。在计算机视觉领域，常用的特征量包括SIFT、SURF和HOG等。使用这些特征量将图像数据转换为向量，然后对转换后的向量使用机器学习中的SVM、KNN等分类器进行学习</p>

<p><img src="/Users/yurunjie/Library/Application Support/typora-user-images/截屏2020-05-10 下午2.12.03.png" alt="截屏2020-05-10 下午2.12.03" style="zoom:50%;" /></p>

<p>深度学习有时也称为端到端机器学习</p>

<p>神经网络的优点是对所有的问题都可以用同样的流程来解决</p>

<h3 id="训练数据和测试数据">训练数据和测试数据</h3>

<p>模型的泛化能力</p>

<p>获得泛化能力时机器学习的最终目标</p>

<p>过拟合</p>

<h2 id="损失函数">损失函数</h2>

<p>神经网络的学习中所用的指标称为损失函数</p>

<p>这个损失函数可以使用任意函数，但一般用均方误差和交叉熵误差等</p>

<h3 id="均方误差">均方误差</h3>

<p>最有名的损失函数 mean squared error</p>

<p><img src="/Users/yurunjie/Library/Application Support/typora-user-images/截屏2020-05-10 下午6.29.44.png" alt="截屏2020-05-10 下午6.29.44" style="zoom:50%;" /></p>

<p>将正确解标签表示为1，其他标签表示为0的表示法成为one-hot表示</p>

<h3 id="交叉熵误差">交叉熵误差</h3>

<p><img src="/Users/yurunjie/Library/Application Support/typora-user-images/截屏2020-05-10 下午6.32.30.png" alt="截屏2020-05-10 下午6.32.30" style="zoom:50%;" /></p>

<p>tk中只有正确解的标签为1，其他均为0。所以实际上只计算对应正确解标签的输出的自然对数</p>

<p>交叉熵误差的值是由正确解标签所对应的输出结果决定的</p>

<p>函数在计算log时，加上微小值delta作为保护性对策，可以避免负无穷大影响后续计算</p>

<h3 id="mini-batch学习">mini-batch学习</h3>

<p>之前的损失函数考虑的都是针对单个数据的损失函数，如果要求所有训练数据的损失函数的总和，以交叉熵为例，可以写成下面的式</p>

<p><img src="/Users/yurunjie/Library/Application Support/typora-user-images/截屏2020-05-10 下午6.44.06.png" alt="截屏2020-05-10 下午6.44.06" style="zoom:50%;" /></p>

<p>除以N以正规化，求单个数据的平均损失函数</p>

<p>通过这样的平均化，可以获得和训练数据的数量无关的统一指标</p>

<p>数据集的训练数据过于庞大，如果以全部数据为对象求损失函数的和，则计算过程需要花费较长的时间甚至是不现实的</p>

<p>因此，我们从全部数据中选出一部分，作为全部数据的近似</p>

<p>神经网络的学习也是从训练数据中选出一部分（称为mini-batch,小批量），然后对每个mini-patch进行学习，这种学习方式称为mini-patch学习</p>

<h3 id="为何要设定损失函数">为何要设定损失函数</h3>

<p>在进行神经网络的学习中，不能将识别精度作为指标。因为如果以识别精度为指标，则参数的导数在绝大多数地方都会变成0</p>

<p>识别精度对于微小的参数变化基本上没有什么反应，即便有反应，它的值也是不连续地、突然地变化。作为激活函数的阶跃函数也有同样的情况。出于相同的原因，如果突然使用阶跃函数作为激活函数，神经网络的学习将无法进行。阶跃函数的导数在绝大多数地方为0。也就是说，如果使用了阶跃函数，那么即便将损失函数作为指标，参数的微小变化也会被阶跃函数抹杀，导致损失函数的值不会产生任何变化</p>

<p>sigmoid函数的导数在任何地方都不为0，这对神经网络的学习非常重要</p>

<h2 id="数值微分">数值微分</h2>

<h3 id="导数">导数</h3>

<p>数值微分含有误差。为了减小这个误差，我们可以计算函数f在（x+h）和（x-h）之间的差分。因为这种计算方法以x为中心，计算它左右两边的差分，所以也称为中心差分</p>

<h2 id="梯度">梯度</h2>

<p>梯度指示的方向是各点处的函数值减小最多的方向</p>

<h3 id="梯度法">梯度法</h3>

<p>一般而言，损失函数很复杂，参数空间庞大，我们不知道他在何处能取得最小值。而通过巧妙地使用梯度来寻找函数最小值（或者尽可能小的值）的方法就是梯度法</p>

<p>这里需要注意的是，无法保证梯度所指的方向就是函数的最小值或真正应该前进的方向。实际上，在复杂的函数中，梯度指示的方向基本上都不是函数值最小处</p>

<p>学习高原：函数很复杂且呈扁平状，学习可能会进入一个几乎平坦的地区</p>

<p>在梯度法中，函数的取值从当前位置沿着梯度方向前进一定距离，然后在新的地方重新求梯度，再沿着新梯度方向前进，如此反复</p>

<p>梯度法是解决机器学习中最优化问题的常用方法，特别是在神经网络的学习中经常被使用</p>

<p><img src="/Users/yurunjie/Library/Application Support/typora-user-images/截屏2020-05-10 下午9.13.03.png" alt="截屏2020-05-10 下午9.13.03" style="zoom:50%;" /></p>

<p>学习率需要事先确定为某个值，比如0.01或0.02。一般而言，这个值过大或过小，都无法抵达一个好的位置。在神经网络的学习中，一般会一边改变学习率的值，一边确认学习是否正确进行了</p>

<p>实验结果表明，学习率过大的话，会发散成一个很大的值；反过来，学习率过小的话，基本上没怎么更新就结束了</p>

<p>超参数：相对于神经网络的权重参数是通过训练数据和学习算法自动获得的，学习率这样的超参数则是人工设定的</p>

<h3 id="神经网络的梯度">神经网络的梯度</h3>

<p><img src="/Users/yurunjie/Library/Application Support/typora-user-images/截屏2020-05-10 下午9.34.13.png" alt="截屏2020-05-10 下午9.34.13" style="zoom:50%;" /></p>

<p>求出神经网络的梯度后，接下来只需根据梯度法，更新权重参数即可</p>

<h2 id="学习算法的实现">学习算法的实现</h2>

<p>神经网络的学习步骤：</p>

<p>1、从训练数据中随机选出一部分数据，这部分数据称为mini-batch。我们的目标是减小mini-batch的损失函数的值</p>

<p>2、计算梯度</p>

<p>3、更新参数</p>

<p>4、重复步骤1、2、3</p>

<p>随机梯度下降法 stochastic gradient descent</p>

<p>如何设置权重参数的初始值这个问题是关系到神经网络能否成功学习的重要问题。后面我们会详细讨论权重参数的初始化，这里只需要知道，权重使用符合高斯分布的随机数进行初始化，偏置使用0进行初始化</p>

<p>epoch是一个单位。一个epoch表示学习中所有训练数据均被使用过一次时的更新次数。比如，对于10000笔训练数据，用大小为100笔数据的mini-batch进行学习时，重复随机梯度下降法100次，所有的训练数据就都被看过了。此时，100次就是一个epoch</p>

<p>可以每经过一个epoch，就对所有的训练数据和测试数据计算识别精度，并记录结果。之所以要计算每一个epoch的识别精度，是因为如果在for语句的循环中一直计算识别精度，会花费太多时间，并且，没什么必要</p>

<h1 id="第五章-误差反向传播法">第五章 误差反向传播法</h1>

<p>数值微分虽然简单，也容易实现，但缺点是计算上比较费时间</p>

<p>误差反向传播法能够高效计算权重参数的梯度</p>

<p>基于数学式 基于计算图</p>

<h2 id="计算图">计算图</h2>

<p>计算图解题的流程</p>

<p>1、构建计算图</p>

<p>2、在计算图上，从左向右进行计算</p>

<p>正向传播</p>

<h3 id="局部计算">局部计算</h3>

<p>计算图可以集中精力于局部计算。无论全局的计算有多么复杂，各个步骤所要做的就是对象节点的局部计算。虽然局部计算非常简单，但是通过传递他的计算结果，可以获得全局的复杂计算的结果</p>

<h3 id="为何用计算图解题">为何用计算图解题</h3>

<p>1、局部计算，简化问题</p>

<p>2、保存中间计算结果</p>

<p>3、可以通过反向传播高效计算导数</p>

<p><img src="/Users/yurunjie/Library/Application Support/typora-user-images/截屏2020-05-10 下午9.58.39.png" alt="截屏2020-05-10 下午9.58.39" style="zoom:50%;" /></p>

<h2 id="链式法则">链式法则</h2>

<p><img src="/Users/yurunjie/Library/Application Support/typora-user-images/截屏2020-05-11 上午10.21.43.png" alt="截屏2020-05-11 上午10.21.43" style="zoom:50%;" /></p>

<p><img src="/Users/yurunjie/Library/Application Support/typora-user-images/截屏2020-05-11 上午10.21.59.png" alt="截屏2020-05-11 上午10.21.59" style="zoom:50%;" /></p>

<h2 id="反向传播">反向传播</h2>

<h3 id="加法节点的反向传播">加法节点的反向传播</h3>

<p><img src="/Users/yurunjie/Library/Application Support/typora-user-images/截屏2020-05-11 上午10.26.12.png" alt="截屏2020-05-11 上午10.26.12" style="zoom:50%;" /></p>

<p><img src="/Users/yurunjie/Library/Application Support/typora-user-images/截屏2020-05-11 上午10.26.54.png" alt="截屏2020-05-11 上午10.26.54" style="zoom:50%;" /></p>

<p><img src="/Users/yurunjie/Library/Application Support/typora-user-images/截屏2020-05-11 上午10.24.49.png" alt="截屏2020-05-11 上午10.24.49" style="zoom:50%;" /></p>

<h3 id="乘法节点的反向传播">乘法节点的反向传播</h3>

<p><img src="/Users/yurunjie/Library/Application Support/typora-user-images/截屏2020-05-11 上午10.27.51.png" alt="截屏2020-05-11 上午10.27.51" style="zoom:50%;" /></p>

<p><img src="/Users/yurunjie/Library/Application Support/typora-user-images/截屏2020-05-11 上午10.25.06.png" alt="截屏2020-05-11 上午10.25.06" style="zoom:50%;" /></p>

<h3 id="苹果的例子">苹果的例子</h3>

<p><img src="/Users/yurunjie/Library/Application Support/typora-user-images/截屏2020-05-11 上午10.29.05.png" alt="截屏2020-05-11 上午10.29.05" style="zoom:50%;" /></p>

<h2 id="简单层的实现">简单层的实现</h2>

<p>计算图的乘法节点称为乘法层，加法节点称为加法层</p>

<p>这里所说的层是神经网络中功能的单元</p>

<h3 id="乘法层的实现">乘法层的实现</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
</pre></td><td class="rouge-code"><pre><span class="k">class</span> <span class="nc">MulLayer</span><span class="p">:</span>
  <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">x</span> <span class="o">=</span> <span class="bp">None</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">x</span> <span class="o">=</span> <span class="bp">None</span>
    
  <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">x</span> <span class="o">=</span> <span class="n">x</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">y</span> <span class="o">=</span> <span class="n">y</span>
    <span class="n">out</span> <span class="o">=</span> <span class="n">x</span> <span class="o">*</span> <span class="n">y</span>
    <span class="n">reurn</span> <span class="n">out</span>
    
  <span class="k">def</span> <span class="nf">backward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dout</span><span class="p">):</span>
    <span class="n">dx</span> <span class="o">=</span> <span class="n">dout</span> <span class="o">*</span> <span class="bp">self</span><span class="p">.</span><span class="n">y</span>
    <span class="n">dy</span> <span class="o">=</span> <span class="n">dout</span> <span class="o">*</span> <span class="bp">self</span><span class="p">.</span><span class="n">x</span>
    <span class="k">return</span> <span class="n">dx</span><span class="p">,</span> <span class="n">dy</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<h3 id="加法层的实现">加法层的实现</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
</pre></td><td class="rouge-code"><pre><span class="k">class</span> <span class="nc">AddLayer</span><span class="p">:</span>
  <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">pass</span>
  
  <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
    <span class="n">out</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="n">y</span>
    <span class="k">return</span> <span class="n">out</span>
  
  <span class="k">def</span> <span class="nf">backward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dout</span><span class="p">):</span>
    <span class="n">dx</span> <span class="o">=</span> <span class="n">dout</span> <span class="o">*</span> <span class="mi">1</span>
    <span class="n">dy</span> <span class="o">=</span> <span class="n">dout</span> <span class="o">*</span> <span class="mi">1</span>
    <span class="k">return</span> <span class="n">dx</span><span class="p">,</span> <span class="n">dy</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<p>加法层不需要特意进行初始化</p>

<h2 id="激活函数层的实现">激活函数层的实现</h2>

<h3 id="relu层">ReLU层</h3>

<p><img src="/Users/yurunjie/Library/Application Support/typora-user-images/截屏2020-05-11 上午10.42.17.png" alt="截屏2020-05-11 上午10.42.17" style="zoom:50%;" /></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
</pre></td><td class="rouge-code"><pre><span class="k">class</span> <span class="nc">Relu</span><span class="p">:</span>
  <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">mask</span> <span class="o">=</span> <span class="bp">None</span>
  
  <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">mask</span> <span class="o">=</span> <span class="p">(</span><span class="n">x</span> <span class="o">&lt;=</span> <span class="mi">0</span><span class="p">)</span>
    <span class="n">out</span> <span class="o">=</span> <span class="n">x</span><span class="p">.</span><span class="n">copy</span><span class="p">()</span>
    <span class="n">out</span><span class="p">[</span><span class="bp">self</span><span class="p">.</span><span class="n">mask</span><span class="p">]</span>  <span class="o">=</span> <span class="mi">0</span>
    <span class="k">return</span> <span class="n">out</span>
  
  <span class="k">def</span> <span class="nf">backward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dout</span><span class="p">):</span>
    <span class="n">dout</span><span class="p">[</span><span class="bp">self</span><span class="p">.</span><span class="n">mask</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">dx</span> <span class="o">=</span> <span class="n">dout</span>
    <span class="k">return</span> <span class="n">dx</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<p>mask是由True/False构成的Numpy数组，它会把正向传播时的输入x的元素中小于等于0的地方保存为True，其他地方保存为False</p>

<h3 id="sigmoid层">Sigmoid层</h3>

<p><img src="/Users/yurunjie/Library/Application Support/typora-user-images/截屏2020-05-11 上午10.48.27.png" alt="截屏2020-05-11 上午10.48.27" style="zoom:50%;" /></p>

<p><img src="/Users/yurunjie/Library/Application Support/typora-user-images/截屏2020-05-11 上午10.48.46.png" alt="截屏2020-05-11 上午10.48.46" style="zoom:50%;" /></p>

<p><img src="/Users/yurunjie/Library/Application Support/typora-user-images/截屏2020-05-11 上午10.49.21.png" alt="截屏2020-05-11 上午10.49.21" style="zoom:50%;" /></p>

<p><img src="/Users/yurunjie/Library/Application Support/typora-user-images/截屏2020-05-11 上午10.49.34.png" alt="截屏2020-05-11 上午10.49.34" style="zoom:50%;" /></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
</pre></td><td class="rouge-code"><pre><span class="k">class</span> <span class="nc">Sigmoid</span><span class="p">:</span>
  <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">out</span> <span class="o">=</span> <span class="bp">None</span>
  
  <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
    <span class="n">out</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">np</span><span class="p">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">x</span><span class="p">))</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">out</span> <span class="o">=</span> <span class="n">out</span>
    <span class="k">return</span> <span class="n">out</span>
  
  <span class="k">def</span> <span class="nf">backward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dout</span><span class="p">):</span>
    <span class="n">dx</span> <span class="o">=</span> <span class="n">dout</span> <span class="o">*</span> <span class="p">(</span><span class="mf">1.0</span> <span class="o">-</span> <span class="bp">self</span><span class="p">.</span><span class="n">out</span><span class="p">)</span> <span class="o">*</span> <span class="bp">self</span><span class="p">.</span><span class="n">out</span>
    <span class="k">return</span> <span class="n">dx</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<h2 id="affinesoftmax层的实现">Affine/Softmax层的实现</h2>

<h3 id="affine层">Affine层</h3>

<p>神经网络的正向传播中进行的矩阵的乘积运算在几何学领域被称为仿射变换。因此，在这里将进行仿射变换的处理实现为Affine层</p>

<p><img src="/Users/yurunjie/Library/Application Support/typora-user-images/截屏2020-05-11 上午11.12.41.png" alt="截屏2020-05-11 上午11.12.41" style="zoom:50%;" /></p>

<p>之前我们见到的计算图的各个节点间流动的是标量，而这个例子中各个节点间传播的是矩阵</p>

<p><img src="/Users/yurunjie/Library/Application Support/typora-user-images/截屏2020-05-11 上午11.50.03.png" alt="截屏2020-05-11 上午11.50.03" style="zoom:50%;" /></p>

<p><img src="/Users/yurunjie/Library/Application Support/typora-user-images/截屏2020-05-11 上午11.51.30.png" alt="截屏2020-05-11 上午11.51.30" style="zoom:50%;" /></p>

<p><img src="/Users/yurunjie/Library/Application Support/typora-user-images/截屏2020-05-11 上午11.52.01.png" alt="截屏2020-05-11 上午11.52.01" style="zoom:50%;" /></p>

<p><img src="/Users/yurunjie/Library/Application Support/typora-user-images/截屏2020-05-11 上午11.52.14.png" alt="截屏2020-05-11 上午11.52.14" style="zoom:50%;" /></p>

<h3 id="批版本的affine层">批版本的Affine层</h3>

<p>前面介绍的Affine层的输入X是以单个数据为对象的。现在我们考虑N个数据一起进行正向传播的情况，也就是批版本的Affine层</p>

<p><img src="/Users/yurunjie/Library/Application Support/typora-user-images/截屏2020-05-11 上午11.53.56.png" alt="截屏2020-05-11 上午11.53.56" style="zoom:50%;" /></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
</pre></td><td class="rouge-code"><pre><span class="k">class</span> <span class="nc">Affine</span><span class="p">:</span>
  <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="n">b</span><span class="p">):</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">W</span> <span class="o">=</span> <span class="n">W</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">b</span> <span class="o">=</span> <span class="n">b</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">x</span> <span class="o">=</span> <span class="bp">None</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">dW</span> <span class="o">=</span> <span class="bp">None</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">db</span> <span class="o">=</span> <span class="bp">None</span>
  
  <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">x</span> <span class="o">=</span> <span class="n">x</span>
    <span class="n">out</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">dot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">W</span><span class="p">)</span> <span class="o">+</span> <span class="bp">self</span><span class="p">.</span><span class="n">b</span>
    <span class="k">return</span> <span class="n">out</span>
  
  <span class="k">def</span> <span class="nf">backward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dout</span><span class="p">):</span>
    <span class="n">dx</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">dot</span><span class="p">(</span><span class="n">dout</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">W</span><span class="p">.</span><span class="n">T</span><span class="p">)</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">dw</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">dot</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">x</span><span class="p">.</span><span class="n">T</span><span class="p">,</span> <span class="n">dout</span><span class="p">)</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">db</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nb">sum</span><span class="p">(</span><span class="n">dout</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">dx</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<h3 id="softmax-with-loss层">Softmax-with-Loss层</h3>

<p>Softmax函数就是将输入值正规化之后再输出</p>

<p>神经网络中进行的处理有推理和学习两个阶段。神经网络的推理通常不使用Softmax层。当神经网络的推理只需要给出一个答案的请光彩，因为此时只对最大得分值感兴趣，所以不需要Softmax层。不过神经网络的学习阶段则需要Softmax层</p>

<p><img src="/Users/yurunjie/Library/Application Support/typora-user-images/截屏2020-05-11 下午1.30.52.png" alt="截屏2020-05-11 下午1.30.52" style="zoom:50%;" /></p>

<p><img src="/Users/yurunjie/Library/Application Support/typora-user-images/截屏2020-05-11 下午1.31.27.png" alt="截屏2020-05-11 下午1.31.27" style="zoom:50%;" /></p>

<p>Softmax函数记为Softmax层，交叉熵误差记为Cross Entropy Error层</p>

<p>这里的结果非常漂亮</p>

<p>神经网络的反向传播会把这个差分表示的误差传递给前面的层，这是神经网络学习中的重要性质</p>

<p>误差大，学到大的内容；误差小，学到小的内容</p>

<p>实际上，这样漂亮的结果并不是偶然的，而是为了得到这样的结果，特意设计了交叉熵误差函数。回归问题中输出层使用恒等函数，损失函数使用平方和误差，也是出于同样的理由。也就是说，使用平方和误差作为恒等函数的损失函数，反向传播才能得到(y1-t1, y2-t2, y3-t3)这样漂亮的结果</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
</pre></td><td class="rouge-code"><pre><span class="k">class</span> <span class="nc">SoftmaxWithLoss</span><span class="p">:</span>
  <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">loss</span> <span class="o">=</span> <span class="bp">None</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">y</span> <span class="o">=</span> <span class="bp">None</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">t</span> <span class="o">=</span> <span class="bp">None</span>
  
  <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">t</span><span class="p">):</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">t</span> <span class="o">=</span> <span class="n">t</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">y</span> <span class="o">=</span> <span class="n">softmax</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">loss</span> <span class="o">=</span> <span class="n">cross_entropy_error</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">y</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">t</span><span class="p">)</span>
    <span class="k">return</span> <span class="bp">self</span><span class="p">.</span><span class="n">loss</span>
  
  <span class="k">def</span> <span class="nf">backward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dout</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
    <span class="n">batch_size</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">t</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">dx</span> <span class="o">=</span> <span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">y</span> <span class="o">-</span> <span class="bp">self</span><span class="p">.</span><span class="n">t</span><span class="p">)</span> <span class="o">/</span> <span class="n">batch_size</span>
    <span class="k">return</span> <span class="n">dx</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<p>注意反向传播时，将要传播的值除以批的大小后，传递给前面的层是单个数据的误差</p>

<h2 id="误差反向传播法的实现">误差反向传播法的实现</h2>

<h3 id="梯度确认">梯度确认</h3>

<p>数值微分的优点是实现简单，因此，一般情况下不太容易出错。而误差反向传播法的实现很复杂，容易出错</p>

<p>确认数值微分求出的梯度结果和误差反向传播法求出的结果是否一致（严格地说。是否非常接近）的操作称为梯度确认</p>

<h1 id="第六章-与学习相关的技巧">第六章 与学习相关的技巧</h1>

<h2 id="参数的更新">参数的更新</h2>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
</pre></td><td class="rouge-code"><pre><span class="k">class</span> <span class="nc">SGD</span><span class="p">:</span>
  <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.01</span><span class="p">):</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">lr</span> <span class="o">=</span> <span class="n">lr</span>
  
  <span class="k">def</span> <span class="nf">update</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">params</span><span class="p">,</span> <span class="n">grads</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="n">params</span><span class="p">.</span><span class="n">keys</span><span class="p">():</span>
      <span class="n">params</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="o">-=</span> <span class="bp">self</span><span class="p">.</span><span class="n">lr</span> <span class="o">*</span> <span class="n">grads</span><span class="p">[</span><span class="n">key</span><span class="p">]</span>
      

<span class="n">network</span> <span class="o">=</span> <span class="n">TwoLayerNet</span><span class="p">(...)</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">SGD</span><span class="p">()</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10000</span><span class="p">):</span>
  <span class="p">...</span>
  <span class="n">x_batch</span><span class="p">,</span> <span class="n">t_batch</span> <span class="o">=</span> <span class="n">get_mini_batch</span><span class="p">(...)</span> <span class="c1">#mini-batch
</span>  <span class="n">grads</span> <span class="o">=</span> <span class="n">network</span><span class="p">.</span><span class="n">gradient</span><span class="p">(</span><span class="n">x_batch</span><span class="p">,</span> <span class="n">t_batch</span><span class="p">)</span>
  <span class="n">params</span> <span class="o">=</span> <span class="n">network</span><span class="p">.</span><span class="n">params</span>
  <span class="n">optimizer</span><span class="p">.</span><span class="n">update</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">grads</span><span class="p">)</span>
  <span class="p">...</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<p>通过单独实现进行最优化的类，功能的模块化变得更简单</p>

<p>这样一来，只需将optimizer = SGD()这一语句换成optimizer = Momentum()，就可以从SGD切换为Momentum</p>

<h3 id="sgd的缺点">SGD的缺点</h3>

<p>SGD简单，且容易实现，但是在解决某些问题时可能没有效率</p>

<p>如果函数的形状非均向，比如呈延伸状，搜索的路径就会非常低效</p>

<p>SGD低效的根本原因是，梯度的方向没有指向最小值的方向</p>

<p><img src="/Users/yurunjie/Library/Application Support/typora-user-images/截屏2020-05-11 下午4.34.03.png" alt="截屏2020-05-11 下午4.34.03" style="zoom:50%;" /></p>

<h3 id="momentum">Momentum</h3>

<p><img src="/Users/yurunjie/Library/Application Support/typora-user-images/截屏2020-05-11 下午4.36.37.png" alt="截屏2020-05-11 下午4.36.37" style="zoom:50%;" /></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
</pre></td><td class="rouge-code"><pre><span class="k">class</span> <span class="nc">Momentum</span><span class="p">:</span>
  <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="mf">0.9</span><span class="p">):</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">lr</span> <span class="o">=</span> <span class="n">lr</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">momentum</span> <span class="o">=</span> <span class="n">momentum</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">V</span> <span class="o">=</span> <span class="bp">None</span>
  <span class="k">def</span> <span class="nf">update</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">params</span><span class="p">,</span> <span class="n">grads</span><span class="p">):</span>
    <span class="k">if</span> <span class="bp">self</span><span class="p">.</span><span class="n">V</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>
      <span class="bp">self</span><span class="p">.</span><span class="n">V</span> <span class="o">=</span> <span class="p">{}</span>
      <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">val</span> <span class="ow">in</span> <span class="n">params</span><span class="p">.</span><span class="n">items</span><span class="p">():</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">V</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">val</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="n">params</span><span class="p">.</span><span class="n">keys</span><span class="p">():</span>
      <span class="bp">self</span><span class="p">.</span><span class="n">V</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">momentum</span><span class="o">*</span><span class="bp">self</span><span class="p">.</span><span class="n">V</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="o">-</span> <span class="bp">self</span><span class="p">.</span><span class="n">lr</span><span class="o">*</span><span class="n">grads</span><span class="p">[</span><span class="n">key</span><span class="p">]</span>
      <span class="n">params</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="o">+=</span> <span class="bp">self</span><span class="p">.</span><span class="n">V</span><span class="p">[</span><span class="n">key</span><span class="p">]</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<p>物体在梯度方向上受力，在这个力的作用下，物体的速度增加这一物理法则</p>

<p>Momentum方法给人的感觉就像是小球在地面上滚动</p>

<p>ɑv这一项，在物体不受任何力时，该项承担使物体逐渐减速的任务（ɑ设定为0.9之类的值），对应物体上的地面摩擦或空气阻力</p>

<p><img src="/Users/yurunjie/Library/Application Support/typora-user-images/截屏2020-05-11 下午4.50.56.png" alt="截屏2020-05-11 下午4.50.56" style="zoom:50%;" /></p>

<p>实例变量v会保存物体的速度</p>

<p>更新路径就像小球在碗中滚动一样。和SGD相比，我们发现之字形的程度减轻了。这是因为虽然x轴方向上的受力很小，但是一直在同一方向上受力，所以朝同一方向会有一定的加速。反过来，虽然y轴方向受力大，但是交互的受到正反方向的力，所以y轴速度不稳定。它可以更快的朝x轴方向靠近</p>

<h3 id="adagrad">AdaGrad</h3>

<p>学习率过小，会导致学习花费过多时间；反过来，学习率过大，则会导致学习发散而不能正确进行</p>

<p>学习率衰减</p>

<p>一开始多学，然后逐渐少学</p>

<p>逐渐减小学习率的想法，相当于将全体参数的学习率值一起降低。而AdaGrad进一步发展了这个想法，针对一个一个的参数，赋予其定制的值</p>

<p>AdaGrad会为参数的每个元素适当的调整学习率，与此同时进行学习</p>

<p><img src="/Users/yurunjie/Library/Application Support/typora-user-images/截屏2020-05-11 下午4.59.46.png" alt="截屏2020-05-11 下午4.59.46" style="zoom:50%;" /></p>

<p>h保存了以前的所有梯度值的平方和</p>

<p>根号h用于调整学习的尺度</p>

<p>参数的元素变动较大（被大幅更新）的元素的学习率将变小</p>

<p>AdaGrad会记录过去所有梯度的平方和。因此，学习越深入，更新的幅度就越小</p>

<p>实际上，如果无止境地学习，更新量就会变为0，完全不再更新</p>

<p>为了改善这个问题，可以使用RMSProp方法，逐渐地遗忘过去的梯度，在做加法运算时将新梯度的信息更多的反映出来</p>

<p>这种操作从专业上讲，称为指数移动平均，呈指数函数式地缩小过去的梯度尺度</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
</pre></td><td class="rouge-code"><pre><span class="k">class</span> <span class="nc">AdaGrad</span><span class="p">:</span>
  <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.01</span><span class="p">):</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">lr</span> <span class="o">=</span> <span class="n">lr</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">h</span> <span class="o">=</span> <span class="bp">None</span>
  
  <span class="k">def</span> <span class="nf">update</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">params</span><span class="p">,</span> <span class="n">grads</span><span class="p">):</span>
    <span class="k">if</span> <span class="bp">self</span><span class="p">.</span><span class="n">h</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>
      <span class="bp">self</span><span class="p">.</span><span class="n">h</span> <span class="o">=</span> <span class="p">{}</span>
      <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">val</span> <span class="ow">in</span> <span class="n">params</span><span class="p">.</span><span class="n">items</span><span class="p">():</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">h</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">val</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="n">params</span><span class="p">.</span><span class="n">keys</span><span class="p">():</span>
    	<span class="bp">self</span><span class="p">.</span><span class="n">h</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="o">+=</span> <span class="n">grads</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="o">*</span> <span class="n">grads</span><span class="p">[</span><span class="n">key</span><span class="p">]</span>
      <span class="n">params</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="o">-=</span> <span class="bp">self</span><span class="p">.</span><span class="n">lr</span> <span class="o">*</span> <span class="n">grads</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="o">/</span> <span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">sqrt</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">h</span><span class="p">[</span><span class="n">key</span><span class="p">])</span> <span class="o">+</span> <span class="mf">1e-7</span><span class="p">)</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<p>微小值用于防止将0用作除数的情况</p>

<p>在很多深度学习的框架中，这个微小值也可以设定为参数</p>

<p><img src="/Users/yurunjie/Library/Application Support/typora-user-images/截屏2020-05-11 下午5.10.47.png" alt="截屏2020-05-11 下午5.10.47" style="zoom:50%;" /></p>

<h3 id="adam">Adam</h3>

<p>理论有些复杂，直观地讲，就是融合了Momentum和AdaGrad的方法</p>

<p>通过组合前面两个方法的优点，有望实现参数空间的高效搜索</p>

<p>此外，进行超参数的偏置校正也是Adam的特征</p>

<p><img src="/Users/yurunjie/Library/Application Support/typora-user-images/截屏2020-05-11 下午5.13.23.png" alt="截屏2020-05-11 下午5.13.23" style="zoom:50%;" /></p>

<p>Adam会设置3个超参数。一个是学习率，另外两个是一次momentum系数和二次momentum系数</p>

<p>根据论文，标准的设定值为0.9和0.999，设置了这些值后，大多数情况下都能顺利运行</p>

<h3 id="更新方法的选择">更新方法的选择</h3>

<p>这4种方法各有各的特点，都有各自擅长解决的问题和不擅长解决的问题</p>

<p>很多研究中至今仍在使用SGD。Momentum和AdaGrad也是值得一试的方法。最近，很多研究人员和技术人员都喜欢用Adam</p>

<h3 id="基于mnist数据集的更新方法的比较">基于MNIST数据集的更新方法的比较</h3>

<p><img src="/Users/yurunjie/Library/Application Support/typora-user-images/截屏2020-05-11 下午5.17.50.png" alt="截屏2020-05-11 下午5.17.50" style="zoom:50%;" /></p>

<p>实验结果会随学习率等超参数、神经网络的结构的不同而发生变化</p>

<p>不过，一般而言，与SGD相比，其他3种方法可以学习地更快，有时最终的识别精度也更高</p>

<h2 id="权重的初始值">权重的初始值</h2>

<p>实际上，设定什么样的权重初始值，经常关系到神经网络的学习能否成功</p>

<h3 id="可以将权重初始值设为0吗">可以将权重初始值设为0吗</h3>

<p>权值衰减：抑制过拟合、提高泛化能力的技巧</p>

<p>一种以减小权重参数的值为目的的进行学习的方法</p>

<p>如果想减小权重的值，一开始就将初始值设为较小的值才是正途。实际上，在这之前的权重参数初始值都是像0.01 * np.random.randn(10, 100)这样，使用由高斯分布生成的值乘以0.01后得到的值（标准差为0.01的高斯分布）</p>

<p>为什么不能将权重初始值设为0呢？严格地说，为什么不能将权重初始值设成一样的值呢？这是因为在误差反向传播法中，所有的权重值都会进行相同的更新</p>

<p>比如，在2层神经网络中，假设第1层和第2层的权重为0。这样一来，正向传播时，因为输入层的权重为0，所以第2层的神经元全部会被传递相同的值。第2层的神经元全部输入相同的值，这意味着反向传播时第2层的权重全部会进行相同的更新.因此，权重被更新为相同的值，并拥有了对称的值。这使得神经网络拥有许多不同的权重的意义丧失了。为了防止权重均以化（严格的来讲，是为了瓦解权重的对称机构），必须随机生成初始值</p>

<h3 id="隐藏层的激活值的分布">隐藏层的激活值的分布</h3>

<p>观察隐藏层的激活值（激活函数的输出数据）的分布，可以获得很多启发</p>

<p>这里，我们来做一个简单的实验，观察权重初始值是如何影响隐藏层的激活值的分布的。这里要做的实验是，向一个5层神经网络（激活函数用的sigmoid函数）传入随机生成的输入数据，用直方图绘制各层激活值的数据分布</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
</pre></td><td class="rouge-code"><pre><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>

<span class="k">def</span> <span class="nf">sigmoid</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
  <span class="k">return</span> <span class="mi">1</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">np</span><span class="p">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">x</span><span class="p">))</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1000</span><span class="p">,</span><span class="mi">10</span><span class="p">)</span> <span class="c1">#1000个数据
</span><span class="n">node_num</span> <span class="o">=</span> <span class="mi">100</span> <span class="c1">#各隐藏层的节点（神经元）数
</span><span class="n">hiddern_layer_size</span> <span class="o">=</span> <span class="mi">5</span> <span class="c1">#隐藏层有5层
</span><span class="n">activations</span> <span class="o">=</span> <span class="p">{}</span> <span class="c1">#激活值的结果保存在这里
</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">hidden_layer_size</span><span class="p">):</span>
  <span class="k">if</span> <span class="n">i</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">activations</span><span class="p">[</span><span class="n">i</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
    
    <span class="n">w</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">randn</span><span class="p">(</span><span class="n">node_num</span><span class="p">,</span> <span class="n">node_num</span><span class="p">)</span> <span class="o">*</span> <span class="mi">1</span>
    
    <span class="n">z</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">dot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">w</span><span class="p">)</span>
    <span class="n">a</span> <span class="o">=</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">z</span><span class="p">)</span> <span class="c1">#sigmoid函数
</span>    <span class="n">activations</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">a</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<p>这个代码中需要注意的是权重的尺度</p>

<p>虽然我们使用的是标准差为1的高斯分布，但实验的目的是通过改变这个尺度（标准差），观察激活值的分布如何变化</p>

<p>现在，我们将保存在activations中的各层数据画成直方图</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
</pre></td><td class="rouge-code"><pre><span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">a</span> <span class="ow">in</span> <span class="n">activations</span><span class="p">.</span><span class="n">items</span><span class="p">():</span>
  <span class="n">plt</span><span class="p">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">activations</span><span class="p">),</span> <span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span>
  <span class="n">plt</span><span class="p">.</span><span class="n">title</span><span class="p">(</span><span class="nb">str</span><span class="p">(</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span> <span class="o">+</span> <span class="s">"-layer"</span><span class="p">)</span>
  <span class="n">plt</span><span class="p">.</span><span class="n">hist</span><span class="p">(</span><span class="n">a</span><span class="p">.</span><span class="n">faltten</span><span class="p">(),</span> <span class="mi">30</span><span class="p">,</span> <span class="nb">range</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">))</span>
<span class="n">plt</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<p><img src="/Users/yurunjie/Library/Application Support/typora-user-images/截屏2020-05-11 下午7.23.53.png" alt="截屏2020-05-11 下午7.23.53" style="zoom:50%;" /></p>

<p>各层的激活值偏向0和1的分布</p>

<p>这里使用的sigmoid函数是S型函数，随着输出不断地靠近0（或者靠近1），他的导数…*y(1-y)的值逐渐接近0。因此，偏向0和1的数据分布会造成反向传播中梯度的值不断变小，最后消失。这个问题称为梯度消失。层次加深的深度学习中，梯度消失的问题可能会更加严重</p>

<p>下面，将权重的标准差设为0.01，进行相同的实验</p>

<p><img src="/Users/yurunjie/Library/Application Support/typora-user-images/截屏2020-05-11 下午8.19.49.png" alt="截屏2020-05-11 下午8.19.49" style="zoom:50%;" /></p>

<p>这次呈集中在0.5附近的分布。而且不像刚才的例子那样偏向0和1，所以不会发生梯度消失的问题。但是，激活值的分布有所偏向，说明在表现力上会有很大的问题</p>

<p>如果有多个神经元都输出几乎相同的值，那么他们就没有存在的意义了。比如，如果100个神经元都输出几乎相同的值，那么也可以由1个神经元来表达基本相同的事情。因此，激活值在分布上有所偏向会出现表现力受限的问题</p>

<p>各层的激活值的分布都要求有适当的广度。因为通过在各层间传递多样性的数据，神经网络可以进行高效的学习。反过来，如果传递的是有所偏向的数据，就会出现梯度消失或者表现力受限的问题，导致学习可能无法顺利进行</p>

<p>我们尝试使用Xavier Glorot等人的论文中推荐的权重初始值（俗称Xavier初始值）</p>

<p>现在，Xavier初始值已被作为标准使用</p>

<p>Xavier的论文中，为了使各层的激活值呈现出具有相同的广度，推导了合适的权重尺度。推导出的结论是，如果前一层的节点数为n，则初始值使用标准差为1/根号n的分布</p>

<p><img src="/Users/yurunjie/Library/Application Support/typora-user-images/截屏2020-05-11 下午8.41.19.png" alt="截屏2020-05-11 下午8.41.19" style="zoom:50%;" /></p>

<p>因为各层间传递的数据有适当的广度，所以sigmoid函数的表现力不受限制，有望进行高效的学习</p>

<p>图6-13的分布中，后面的层的分布呈稍微歪斜的形状。如果用tanh函数（双曲线函数）代替sigmoid函数，这个稍微歪斜的问题就能得到改善。实际上，使用tanh函数后，会呈漂亮的吊钟型分布。tanh函数和sigmoid函数同是S型双曲线函数，但tanh函数是关于原点(0,0)对称的S型曲线函数，但sigmoid函数是关于(x,y)=(0,0.5)对称的S型曲线</p>

<p>众所周知，用作激活函数的函数最好具有关于原点对称的性质</p>

<h3 id="relu的权重初始值">ReLU的权重初始值</h3>

<p>Xavier初始值是以激活函数是线性函数为前提而推导出来的</p>

<p>因为sigmoid函数和tanh函数左右对称，且中央附近可以视作线性函数，所以适合使用Xavier初始值</p>

<p>但当激活函数使用ReLU时，一般推荐使用ReLU专用的初始值，也就是Kaimming He等人推荐的初始值，也称为He初始值</p>

<p>当前一层的节点数为n时，He初始值使用标准差为根号(2/n)的高斯分布</p>

<p>直观上可以解释为，因为ReLU的负值区域的值为0，为了使它更有广度，所以需要二倍的系数</p>

<p><img src="/Users/yurunjie/Library/Application Support/typora-user-images/截屏2020-05-11 下午8.52.49.png" alt="截屏2020-05-11 下午8.52.49" style="zoom:50%;" /></p>

<p>观察实验结果可知，当std=0.01时，各层的激活值非常小。神经网络上传递的是非常小的值，说明逆向传播时权重的梯度也同样很小。这是很严重的问题，实际上学习基本上没有进展</p>

<p>接下来是初始值为Xavier初始值时的结果。在这种情况下，随着层的加深，偏向一点点变大。实际上，层加深后，激活值的偏向变大，学习时会出现梯度消失的问题。而当初始值为He初始值时，各层中分布的广度相同。由于即便层加深，数据的广度也能保持不变，因此逆向传播时，也会传递合适的值</p>

<p>总结一下，当激活函数使用ReLU时，权重初始值使用He初始值，当激活函数为sigmoid或tanh等S型曲线函数时，初始值使用Xavier初始值，这是目前的最佳实践</p>

<h3 id="基于mnist数据集的权重初始值的比较">基于MNIST数据集的权重初始值的比较</h3>

<p><img src="/Users/yurunjie/Library/Application Support/typora-user-images/截屏2020-05-11 下午9.24.00.png" alt="截屏2020-05-11 下午9.24.00" style="zoom:50%;" /></p>

<p>由图中的结果可知，std=0.01时完全无法进行学习。这和刚才观察到的激活值的分布一样，是因为正向传播中传递的值很小（集中在0附近的数据）。因此，逆向传播时求到的梯度也很小，权重几乎不进行更新。相反，当权重初始值为Xavier初始值和He初始值时，学习进行的很顺利。并且，我们发现He初始值时学习进度更快一些</p>

<p>综上，在神经网络的学习中，权重初始值非常重要。很多时候权重初始值的设定关系到神经网络的学习能否成功</p>

<h2 id="batch-normalization">Batch Normalization</h2>

<p>为了使各层拥有适当的广度，强制性地调整激活值的分布会怎样呢？</p>

<p>实际上，Batch Normalization方法就是基于这个想法而产生的</p>

<h3 id="batch-normalization的算法">Batch Normalization的算法</h3>

<p>Batch Norm是2015年提出的方法。虽然是一个问世不久的新方法，但已经被很多研究人员和技术人员广泛使用。实际上，看一下机器学习竞赛的结果，就会发现很多通过使用这个方法而获得优异结果的例子</p>

<p>优点</p>

<p>1、可以使学习快速进行（可以增大学习率）</p>

<p>2、不那么依赖初始值（对于初始值不用那么神经质）</p>

<p>3、抑制过拟合（降低Dropout等的必要性）</p>

<p>Batch Norm的思路是调整各层的激活值分布使其拥有适当的广度。为此，要向神经网络中插入对数据分布进行正规化的层，即Batch Norm层</p>

<p><img src="/Users/yurunjie/Library/Application Support/typora-user-images/截屏2020-05-11 下午9.54.30.png" alt="截屏2020-05-11 下午9.54.30" style="zoom:50%;" /></p>

<p>Batch Norm，以进行学习时的mini-batch为单位，按mini-batch进行正规化</p>

<p>具体而言，就是进行使数据分布的均值为0、方差为1的正规化</p>

<p><img src="/Users/yurunjie/Library/Application Support/typora-user-images/截屏2020-05-11 下午9.56.15.png" alt="截屏2020-05-11 下午9.56.15" style="zoom:50%;" /></p>

<p>这里对mini-batch的m个输入数据的集合求均值和方差。然会，对输入数据进行均值为0、方差为1（合适的分布）的正规化。式中的微小值是为了防止除以0的情况</p>

<p>通过将这个处理插入到激活函数的前面或后面，可以减小数据分布的偏向</p>

<p>接着，Batch Norm层会对正规化的数据进行缩放和平移的变换，用数学式可以如下表示</p>

<p><img src="/Users/yurunjie/Library/Application Support/typora-user-images/截屏2020-05-11 下午10.00.36.png" alt="截屏2020-05-11 下午10.00.36" style="zoom:50%;" /></p>

<h3 id="batch-normalization的评估">Batch Normalization的评估</h3>

<p><img src="/Users/yurunjie/Library/Application Support/typora-user-images/截屏2020-05-11 下午10.01.45.png" alt="截屏2020-05-11 下午10.01.45" style="zoom:50%;" /></p>

<p>基于不同的初始值尺度，观察学习的情况如何变化</p>

<p><img src="/Users/yurunjie/Library/Application Support/typora-user-images/截屏2020-05-11 下午10.02.33.png" alt="截屏2020-05-11 下午10.02.33" style="zoom:50%;" /></p>

<p>几乎所有的情况下都是使用Batch Norm时学习进行的更快</p>

<p>实际上，在不使用Batch Norm的情况下，如果不赋予一个尺度好的初始值，学习将无法进行</p>

<p>使用Batch Norm，可以推动学习的进行。并且，使权重初始值变得健壮</p>

<h2 id="正则化">正则化</h2>

<h3 id="过拟合">过拟合</h3>

<p>发生过拟合的原因，主要有以下两个</p>

<p>模型拥有大量参数、表现力强</p>

<p>训练数据少</p>

<h3 id="权值衰减">权值衰减</h3>

<p>权值衰减是一直以来经常被使用的一种抑制过拟合的方法。该方法通过在学习的过程中对大的权重进行惩罚，来抑制过拟合</p>

<p>很多过拟合原本就是因为权重参数取值过大才发生的</p>

<p>神经网络的学习目的是减小损失函数的值。这时，例如为损失函数加上权重的平方范数（L2范数）。这样一来，就可以抑制权重变大</p>

<p><img src="/Users/yurunjie/Library/Application Support/typora-user-images/截屏2020-05-11 下午11.11.40.png" alt="截屏2020-05-11 下午11.11.40" style="zoom:50%;" /></p>

<p><img src="/Users/yurunjie/Library/Application Support/typora-user-images/截屏2020-05-11 下午11.12.08.png" alt="截屏2020-05-11 下午11.12.08" style="zoom:50%;" /></p>

<p><img src="/Users/yurunjie/Library/Application Support/typora-user-images/截屏2020-05-11 下午11.13.10.png" alt="截屏2020-05-11 下午11.13.10" style="zoom:50%;" /></p>

<p><img src="/Users/yurunjie/Library/Application Support/typora-user-images/截屏2020-05-11 下午11.12.50.png" alt="截屏2020-05-11 下午11.12.50" style="zoom:50%;" /></p>

<h3 id="dropout">Dropout</h3>

<p>作为抑制过拟合的方法，前面我们介绍了为损失函数加上权重的L2范数的权值衰减方法</p>

<p>该方法可以简单的实现，在某种程度上能够抑制过拟合</p>

<p>但是，如果神经网络的模型变得很复杂，只用权值衰减就难以应对了</p>

<p>在这种情况下，我们经常会使用Dropout方法</p>

<p>Dropout是一种在学习的过程中随机删除神经元的方法。训练时，随机选出隐藏层的神经元，然后将其删除。被删除的神经元不再进行信号的传递。训练时，每传递一次数据，就会随机选择要删除的神经元。然后，测试时，虽然会传递所有的神经元信号，但是对于各个神经元的输出，要乘上训练时的删除比例后再输出</p>

<p><img src="/Users/yurunjie/Library/Application Support/typora-user-images/截屏2020-05-11 下午11.18.21.png" alt="截屏2020-05-11 下午11.18.21" style="zoom:50%;" /></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
</pre></td><td class="rouge-code"><pre><span class="k">class</span> <span class="nc">Dropout</span><span class="p">:</span>
  <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dropout_ratio</span><span class="o">=</span><span class="mf">0.5</span><span class="p">):</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">dropout_ratio</span> <span class="o">=</span> <span class="n">dropout_ratio</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">mask</span> <span class="o">=</span> <span class="bp">None</span>
  
  <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">train_flg</span><span class="o">=</span><span class="bp">True</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">train_flg</span><span class="p">:</span>
      <span class="bp">self</span><span class="p">.</span><span class="n">mask</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">rand</span><span class="p">(</span><span class="o">*</span><span class="n">x</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">&gt;</span> <span class="n">sekf</span><span class="p">.</span><span class="n">dropout_ratio</span>
      <span class="k">return</span> <span class="n">x</span> <span class="o">*</span> <span class="bp">self</span><span class="p">.</span><span class="n">mask</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="k">return</span> <span class="n">x</span> <span class="o">*</span> <span class="p">(</span><span class="mf">1.0</span> <span class="o">-</span> <span class="bp">self</span><span class="p">.</span><span class="n">dropout_ratio</span><span class="p">)</span>
  
  <span class="k">def</span> <span class="nf">backward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dout</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">dout</span> <span class="o">*</span> <span class="bp">self</span><span class="p">.</span><span class="n">mask</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<p><img src="/Users/yurunjie/Library/Application Support/typora-user-images/截屏2020-05-11 下午11.23.26.png" alt="截屏2020-05-11 下午11.23.26" style="zoom:50%;" /></p>

<p><img src="/Users/yurunjie/Library/Application Support/typora-user-images/截屏2020-05-11 下午11.23.47.png" alt="截屏2020-05-11 下午11.23.47" style="zoom:50%;" /></p>

<p>通过使用Dropout，即便是表现力强的网络，也可以抑制过拟合</p>

<p>机器学习中经常使用集成学习。所谓集成学习，就是让多个模型单独进行学习，推理时再取多个模型的输出的平均值。实验告诉我们，通过进行集成学习，神经网络的识别精度可以提高好几个百分点</p>

<p>这个集成学习与Dropout有密切的关系。这是因为可以讲Dropout理解为，通过在学习过程中随机删除神经元，从而每一次都让不同的模型进行学习。并且，推理时，通过对神经元的输出乘以删除比例，可以取得模型的平均值。也就是说，可以理解成，Dropout将集成学习的效果（模拟地）通过一个网络实现了</p>

<h2 id="超参数的验证">超参数的验证</h2>

<p>这里所说的超参数是指，比如各层的神经元数量、batch大小、参数更新时的学习率或权值衰减等。如果这些超参数没有设置合适的值，模型的性能就会很差</p>

<p>虽然超参数的取值非常重要，但是在决定超参数的过程中一般会伴随很多的试错</p>

<h3 id="验证数据">验证数据</h3>

<p>不能用测试数据评估超参数的性能</p>

<p>这是因为如果使用测试数据调整超参数，超参数的值会对测试数据发生过拟合</p>

<p>训练数据用于参数（权重和偏置）的学习，验证数据用于超参数的性能评估。为了确认泛化能力，要在最后使用（比较理想的是只用一次）测试数据</p>

<p><img src="/Users/yurunjie/Library/Application Support/typora-user-images/截屏2020-05-11 下午11.35.06.png" alt="截屏2020-05-11 下午11.35.06" style="zoom:50%;" /></p>

<h3 id="超参数的最优化">超参数的最优化</h3>

<p>进行超参数的最优化时，逐渐缩小超参数的好值的存在范围非常重要</p>

<p>有报告显示，在进行神经网络的超参数的最优化时，与网格搜索等有规律的搜索相比，随机采样的搜索方式效果最好。这是因为在多个超参数中，各个超参数对最终的识别精度的影响程度不同</p>

<p>超参数的范围只需大致的制定就可以了</p>

<p>在超参数的最优化中，要注意的是深度学习需要很长时间（比如，几天或几周）</p>

<p>因此，在超参数的搜索中，需要尽早放弃那些不符合逻辑的超参数。于是，在超参数的最优化中，减少学习的epoch，缩短一次评估所需的时间是一个不错的方法</p>

<p>0、设定超参数的范围</p>

<p>1、从设定的超参数范围中随机采样</p>

<p>2、使用步骤1中采样到的超参数的值进行学习，通过验证数据评估识别精度（但是要将epoch设置的很小）</p>

<p>3、重复步骤1和步骤2（100次等），根据他们的识别精度的结果，缩小超参数的范围</p>

<p>反复进行上述操作，不断缩小超参数的范围，在缩小到一定程度时，从该范围中选出一个超参数的值。这就是进行超参数的最优化的一种方法</p>

<p><img src="/Users/yurunjie/Library/Application Support/typora-user-images/截屏2020-05-11 下午11.44.07.png" alt="截屏2020-05-11 下午11.44.07" style="zoom:50%;" /></p>

<h3 id="超参数最优化的实现">超参数最优化的实现</h3>

<p><img src="/Users/yurunjie/Library/Application Support/typora-user-images/截屏2020-05-11 下午11.45.26.png" alt="截屏2020-05-11 下午11.45.26" style="zoom:50%;" /></p>


                <hr style="visibility: hidden;">
                <ul class="pager">
                    
                    <li class="previous">
                        <a href="/2020/09/15/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" data-toggle="tooltip" data-placement="top" title="深度学习入门：基于Python的理论与实现（3）">
                        Previous<br>
                        <span>深度学习入门：基于Python的理论与实现（3）</span>
                        </a>
                    </li>
                    
                    
                    <li class="next">
                        <a href="/2020/09/15/LeetCode-%E7%AC%AC%E4%B8%80%E5%91%A8/" data-toggle="tooltip" data-placement="top" title="刷LeetCode（第一周">
                        Next<br>
                        <span>刷LeetCode（第一周</span>
                        </a>
                    </li>
                    
                </ul>
                <hr style="visibility: hidden;">

                

                
            </div>  

    <!-- Side Catalog Container -->
        
            <div class="
                col-lg-2 col-lg-offset-0
                visible-lg-block
                sidebar-container
                catalog-container">
                <div class="side-catalog">
                    <hr class="hidden-sm hidden-xs">
                    <h5>
                        <a class="catalog-toggle" href="#">CATALOG</a>
                    </h5>
                    <ul class="catalog-body"></ul>
                </div>
            </div>
        

    <!-- Sidebar Container -->
            <div class="
                col-lg-8 col-lg-offset-2
                col-md-10 col-md-offset-1
                sidebar-container">

                <!-- Featured Tags -->
                


<section>
    
        <hr class="hidden-sm hidden-xs">
    
    <h5><a href="/archive/">FEATURED TAGS</a></h5>
    <div class="tags">
        
        
        
        
        
        
                <a data-sort="0005" 
                    href="/archive/?tag=%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0"
                    title="学习笔记"
                    rel="13">学习笔记</a>
        
                <a data-sort="0012" 
                    href="/archive/?tag=%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C"
                    title="计算机网络"
                    rel="6">计算机网络</a>
        
                <a data-sort="0012" 
                    href="/archive/?tag=%E8%B0%A2%E5%B8%8C%E4%BB%81"
                    title="谢希仁"
                    rel="6">谢希仁</a>
        
                <a data-sort="0013" 
                    href="/archive/?tag=Python"
                    title="Python"
                    rel="5">Python</a>
        
                <a data-sort="0014" 
                    href="/archive/?tag=tips"
                    title="tips"
                    rel="4">tips</a>
        
                <a data-sort="0015" 
                    href="/archive/?tag=%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0"
                    title="深度学习"
                    rel="3">深度学习</a>
        
                <a data-sort="0015" 
                    href="/archive/?tag=AI"
                    title="AI"
                    rel="3">AI</a>
        
                <a data-sort="0016" 
                    href="/archive/?tag=%E7%88%AC%E8%99%AB"
                    title="爬虫"
                    rel="2">爬虫</a>
        
                <a data-sort="0016" 
                    href="/archive/?tag=%E7%AE%97%E6%B3%95"
                    title="算法"
                    rel="2">算法</a>
    </div>
</section>


                <!-- Friends Blog -->
                
<hr>
<h5>FRIENDS</h5>
<ul class="list-inline">
  
  <li><a href="https://rapiz.me">rapiz</a></li>
  
</ul>

            </div>
        </div>
    </div>
</article>

<!-- add support for mathjax by voleking-->









<!-- async load function -->
<script>
    function async(u, c) {
      var d = document, t = 'script',
          o = d.createElement(t),
          s = d.getElementsByTagName(t)[0];
      o.src = u;
      if (c) { o.addEventListener('load', function (e) { c(null, e); }, false); }
      s.parentNode.insertBefore(o, s);
    }
</script>
<!-- anchor-js, Doc:http://bryanbraun.github.io/anchorjs/ -->
<script>
    async("//cdnjs.cloudflare.com/ajax/libs/anchor-js/1.1.1/anchor.min.js",function(){
        anchors.options = {
          visible: 'hover',
          placement: 'right',
          // icon: '#'
        };
        anchors.add().remove('.intro-header h1').remove('.subheading').remove('.sidebar-container h5');
    })
</script>
<style>
    /* place left on bigger screen */
    @media all and (min-width: 800px) {
        .anchorjs-link{
            position: absolute;
            left: -0.75em;
            font-size: 1.1em;
            margin-top : -0.1em;
        }
    }
</style>



    <!-- Footer -->
<footer>
    <div class="container">
        <div class="row">
            <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
                <!-- SNS Link -->
                


<ul class="list-inline text-center">


  
  
  
  
  
  
  <li>
    <a target="_blank" href="https://github.com/yrj-creator">
      <span class="fa-stack fa-lg">
        <i class="fa fa-circle fa-stack-2x"></i>
        <i class="fa fa-github fa-stack-1x fa-inverse"></i>
      </span>
    </a>
  </li>
  
  
</ul>

                <p class="copyright text-muted">
                    Copyright &copy; Yurj Blog 2020
                    <br>
                    Powered by <a href="http://huangxuan.me">Hux Blog</a> |
                    <iframe style="margin-left: 2px; margin-bottom:-5px;" frameborder="0" scrolling="0" width="100px"
                        height="20px"
                        src="https://ghbtns.com/github-btn.html?user=huxpro&repo=huxpro.github.io&type=star&count=true">
                    </iframe>
                </p>
            </div>
        </div>
    </div>
</footer>

<!-- jQuery -->
<script src="/js/jquery.min.js "></script>

<!-- Bootstrap Core JavaScript -->
<!-- Currently, only navbar scroll-down effect at desktop still depends on this -->
<script src="/js/bootstrap.min.js "></script>

<!-- Custom Theme JavaScript -->
<script src="/js/hux-blog.min.js "></script>

<!-- Simple Jekyll Search -->
<script src="/js/simple-jekyll-search.min.js"></script>

<!-- Service Worker -->

<script src="/js/snackbar.js "></script>
<script src="/js/sw-registration.js "></script>


<!-- async load function -->
<script>
    function async(u, c) {
        var d = document, t = 'script',
            o = d.createElement(t),
            s = d.getElementsByTagName(t)[0];
        o.src = u;
        if (c) { o.addEventListener('load', function (e) { c(null, e); }, false); }
        s.parentNode.insertBefore(o, s);
    }
</script>

<!--
     Because of the native support for backtick-style fenced code blocks
     right within the Markdown is landed in Github Pages,
     From V1.6, There is no need for Highlight.js,
     so Huxblog drops it officially.

     - https://github.com/blog/2100-github-pages-now-faster-and-simpler-with-jekyll-3-0
     - https://help.github.com/articles/creating-and-highlighting-code-blocks/
     - https://github.com/jneen/rouge/wiki/list-of-supported-languages-and-lexers
-->
<!--
    <script>
        async("http://cdn.bootcss.com/highlight.js/8.6/highlight.min.js", function(){
            hljs.initHighlightingOnLoad();
        })
    </script>
    <link href="http://cdn.bootcss.com/highlight.js/8.6/styles/github.min.css" rel="stylesheet">
-->







<!--fastClick.js -->
<script>
    async("//cdnjs.cloudflare.com/ajax/libs/fastclick/1.0.6/fastclick.min.js", function () {
        var $nav = document.querySelector("nav");
        if ($nav) FastClick.attach($nav);
    })
</script>


<!-- Google Analytics -->



<!-- Baidu Tongji -->



<!-- Side Catalog -->

<script type="text/javascript">
    function generateCatalog(selector) {

        // interop with multilangual 
        if ('' == 'true') {
            _containerSelector = 'div.post-container.active'
        } else {
            _containerSelector = 'div.post-container'
        }

        // init
        var P = $(_containerSelector), a, n, t, l, i, c;
        a = P.find('h1,h2,h3,h4,h5,h6');

        // clean
        $(selector).html('')

        // appending
        a.each(function () {
            n = $(this).prop('tagName').toLowerCase();
            i = "#" + $(this).prop('id');
            t = $(this).text();
            c = $('<a href="' + i + '" rel="nofollow">' + t + '</a>');
            l = $('<li class="' + n + '_nav"></li>').append(c);
            $(selector).append(l);
        });
        return true;
    }

    generateCatalog(".catalog-body");

    // toggle side catalog
    $(".catalog-toggle").click((function (e) {
        e.preventDefault();
        $('.side-catalog').toggleClass("fold")
    }))

    /*
     * Doc: https://github.com/davist11/jQuery-One-Page-Nav
     * Fork by Hux to support padding
     */
    async("/js/jquery.nav.js", function () {
        $('.catalog-body').onePageNav({
            currentClass: "active",
            changeHash: !1,
            easing: "swing",
            filter: "",
            scrollSpeed: 700,
            scrollOffset: 0,
            scrollThreshold: .2,
            begin: null,
            end: null,
            scrollChange: null,
            padding: 80
        });
    });
</script>



<!-- Multi-Lingual -->


<!-- Simple Jekyll Search -->
<script>
    // https://stackoverflow.com/questions/1912501/unescape-html-entities-in-javascript
    function htmlDecode(input) {
        var e = document.createElement('textarea');
        e.innerHTML = input;
        // handle case of empty input
        return e.childNodes.length === 0 ? "" : e.childNodes[0].nodeValue;
    }

    SimpleJekyllSearch({
        searchInput: document.getElementById('search-input'),
        resultsContainer: document.getElementById('search-results'),
        json: '/search.json',
        searchResultTemplate: '<div class="post-preview item"><a href="{url}"><h2 class="post-title">{title}</h2><h3 class="post-subtitle">{subtitle}</h3><hr></a></div>',
        noResultsText: 'No results',
        limit: 50,
        fuzzy: false,
        // a hack to get escaped subtitle unescaped. for some reason, 
        // post.subtitle w/o escape filter nuke entire search.
        templateMiddleware: function (prop, value, template) {
            if (prop === 'subtitle' || prop === 'title') {
                if (value.indexOf("code")) {
                    return htmlDecode(value);
                } else {
                    return value;
                }
            }
        }
    });

    $(document).ready(function () {
        var $searchPage = $('.search-page');
        var $searchOpen = $('.search-icon');
        var $searchClose = $('.search-icon-close');
        var $searchInput = $('#search-input');
        var $body = $('body');

        $searchOpen.on('click', function (e) {
            e.preventDefault();
            $searchPage.toggleClass('search-active');
            var prevClasses = $body.attr('class') || '';
            setTimeout(function () {
                $body.addClass('no-scroll');
            }, 400)

            if ($searchPage.hasClass('search-active')) {
                $searchClose.on('click', function (e) {
                    e.preventDefault();
                    $searchPage.removeClass('search-active');
                    $body.attr('class', prevClasses);  // from closure 
                });
                $searchInput.focus();
            }
        });
    });
</script>


<!-- Image to hack wechat -->
<img src="/img/icon_wechat.png" width="0" height="0" />
<!-- Migrate from head to bottom, no longer block render and still work -->

</body>

</html>
